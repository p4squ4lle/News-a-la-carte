{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conventional Approaches to Recommendation\n",
    "\n",
    "In this notebook, we will build our first recommender systems, based on a subset of our data. We start by constructing a **content based filtering model using the articles' titles and abstract with a tf-idf algorithm**. We will then undertake a **matrix factorization, using reader-artcile interactions**. After that, we will also build a **hybrid filtering model**.\n",
    "\n",
    "Let's first import some libraries and modules and also load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nf/lib/python3.6/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn('LightFM was compiled without OpenMP support. '\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "from lightfm import LightFM\n",
    "from lightfm.data import Dataset\n",
    "import lightfm as lm\n",
    "from lightfm import cross_validation \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from lightfmHelper import evaluate\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors = pd.read_csv(\"../../data/mind_small_train/behaviors_processed_small.csv\")\n",
    "news = pd.read_csv(\"../../data/mind_small_train/news_processed_small.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will only be working with the reading histories in this notebook (which are the same for all the sessions), let's make them to a list and also **drop mutliple user sessions**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors.history = behaviors.history.str.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors.drop_duplicates(subset=\"user_id\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content based filtering with tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we want to give recommendations based on content. We do have some information on articles' categories, subcategories etc., but here we want to build **a system that utilizes textual information**. Let s first combine all the text available for each article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['news_text'] = news['title'] + ' ' + news['abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so that we can build a representation for every individual article based on the words that appear in it. We can use scikit-learn's tf-idf Vectorizer for this task, which doesn't only **let us build a matrix containing all the articles and some kind of one hot encoding for all of the words appearing in our text corpus** (except for some pre-defined english stopwords), but also **weighs the words according to their appearance in the specific article against their appearance in the whole corpus**. The assumption for this procedure is that on the one side, the more frequent an expression in a *specific* document is, the more important this expression should be for the character of the document, on the other side, the more frequent the expression is in the *whole* corpus, the less valuable it should be to single out the individual nature of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_matrix= tfidf.fit_transform(news['news_text'].apply(lambda x: np.str_(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50434, 54324), 50434)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_matrix.shape, news.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a matrix that contains all our articles and represents them in a space with as many dimensions as there are individual linguistic expressions (except for stop words again) in titles and abstracts. With this, we can now construct a **similarity matrix** with the help of scikit-learns linear kernel, which basically calculates the dot product for every pair of articles, so that we can say **how close each article linguistically is to each other**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50434, 50434)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix = linear_kernel(text_matrix,text_matrix)\n",
    "similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make a mapping that enables us to **get the index in our similarity matrix from an article ID**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50429"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_id = pd.Series(news.index,index = news['article_id'])\n",
    "mapping_id['N16909']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "      <th>news_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50429</th>\n",
       "      <td>N16909</td>\n",
       "      <td>weather</td>\n",
       "      <td>weathertopstories</td>\n",
       "      <td>Adapting, Learning And Soul Searching: Reflect...</td>\n",
       "      <td>Woolsey Fire Anniversary: A community is forev...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/BBWzQJK.html</td>\n",
       "      <td>[{\"Label\": \"Woolsey Fire\", \"Type\": \"N\", \"Wikid...</td>\n",
       "      <td>[{\"Label\": \"Woolsey Fire\", \"Type\": \"N\", \"Wikid...</td>\n",
       "      <td>Adapting, Learning And Soul Searching: Reflect...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id category        subcategory  \\\n",
       "50429     N16909  weather  weathertopstories   \n",
       "\n",
       "                                                   title  \\\n",
       "50429  Adapting, Learning And Soul Searching: Reflect...   \n",
       "\n",
       "                                                abstract  \\\n",
       "50429  Woolsey Fire Anniversary: A community is forev...   \n",
       "\n",
       "                                                 url  \\\n",
       "50429  https://assets.msn.com/labs/mind/BBWzQJK.html   \n",
       "\n",
       "                                          title_entities  \\\n",
       "50429  [{\"Label\": \"Woolsey Fire\", \"Type\": \"N\", \"Wikid...   \n",
       "\n",
       "                                       abstract_entities  \\\n",
       "50429  [{\"Label\": \"Woolsey Fire\", \"Type\": \"N\", \"Wikid...   \n",
       "\n",
       "                                               news_text  \n",
       "50429  Adapting, Learning And Soul Searching: Reflect...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_check = news[news.article_id == 'N16909']\n",
    "article_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also one that gets the **article's title from it's ID**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I Was An NBA Wife. Here's How It Affected My Mental Health.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_for_title = news.set_index('article_id')\n",
    "mapping_title = pd.Series(news_for_title.title)\n",
    "mapping_title['N53526']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "      <th>news_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N53526</td>\n",
       "      <td>health</td>\n",
       "      <td>voices</td>\n",
       "      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n",
       "      <td>I felt like I was a fraud, and being an NBA wi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AACk2N6.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"National Basketball Association\", ...</td>\n",
       "      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id category subcategory  \\\n",
       "3     N53526   health      voices   \n",
       "\n",
       "                                               title  \\\n",
       "3  I Was An NBA Wife. Here's How It Affected My M...   \n",
       "\n",
       "                                            abstract  \\\n",
       "3  I felt like I was a fraud, and being an NBA wi...   \n",
       "\n",
       "                                             url title_entities  \\\n",
       "3  https://assets.msn.com/labs/mind/AACk2N6.html             []   \n",
       "\n",
       "                                   abstract_entities  \\\n",
       "3  [{\"Label\": \"National Basketball Association\", ...   \n",
       "\n",
       "                                           news_text  \n",
       "3  I Was An NBA Wife. Here's How It Affected My M...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_check_2 = news[news.article_id == 'N53526']\n",
    "article_check_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to write a function that gives us the **k linguistically closest articles**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommended_articles(news_id, k=10):\n",
    "    news_index = mapping_id[news_id]\n",
    "    similarity_score = list(enumerate(similarity_matrix[news_index]))\n",
    "    similarity_score = sorted(similarity_score, key=lambda x: x[1], reverse=True)\n",
    "    similarity_score = similarity_score[1:k+1]\n",
    "    news_indices = [i[0] for i in similarity_score]\n",
    "    return (news['article_id'].iloc[news_indices].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N48828', 'N13856', 'N16308', 'N33131', 'N23206', 'N389', 'N1634',\n",
       "       'N34192', 'N29952', 'N41317'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_articles('N24217')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a **user-lookup-table**, so that we can write a **function that gives recommendations based on actual user behavior**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_lookup = behaviors.set_index('user_id').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recs_tfidf_first(user_id):\n",
    "    used_article = user_lookup.loc[user_id].history[0]\n",
    "    recs_for_article = recommended_articles(used_article)\n",
    "    article_titles = []\n",
    "    \n",
    "    for article in recs_for_article:\n",
    "        article_titles.append(mapping_title[article])\n",
    "    \n",
    "    \n",
    "    hits = 0\n",
    "    for article in recs_for_article:\n",
    "        if article in user_lookup.loc[user_id].history:\n",
    "            hits += 1\n",
    "    \n",
    "    print(f'The first read article of user {user_id} was: \\n {mapping_title[used_article]}')\n",
    "    print('_______________________________________________________')\n",
    "    print(f'The suggested articles are:\\n')\n",
    "    for article in article_titles:\n",
    "        print(article) \n",
    "    print(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>time</th>\n",
       "      <th>history</th>\n",
       "      <th>impressions</th>\n",
       "      <th>length_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U13740</td>\n",
       "      <td>11/11/2019 9:05:58 AM</td>\n",
       "      <td>[N55189, N42782, N34694, N45794, N18445, N6330...</td>\n",
       "      <td>N55689-1 N35729-0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U91836</td>\n",
       "      <td>11/12/2019 6:11:30 PM</td>\n",
       "      <td>[N31739, N6072, N63045, N23979, N35656, N43353...</td>\n",
       "      <td>N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impression_id user_id                   time  \\\n",
       "0              1  U13740  11/11/2019 9:05:58 AM   \n",
       "1              2  U91836  11/12/2019 6:11:30 PM   \n",
       "\n",
       "                                             history  \\\n",
       "0  [N55189, N42782, N34694, N45794, N18445, N6330...   \n",
       "1  [N31739, N6072, N63045, N23979, N35656, N43353...   \n",
       "\n",
       "                                         impressions  length_history  \n",
       "0                                  N55689-1 N35729-0               9  \n",
       "1  N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...              82  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first read article of user U13740 was: \n",
      " 'Wheel Of Fortune' Guest Delivers Hilarious, Off The Rails Introduction\n",
      "_______________________________________________________\n",
      "The suggested articles are:\n",
      "\n",
      "Best Response Ever From a 'Wheel of Fortune' Contestant?\n",
      "Viral Wheel of Fortune Contestant and His Wife Clarify Hilarious 'Loveless Marriage' Intro\n",
      "'Wheel Of Fortune' Host Pat Sajak Recovers After Surgery\n",
      "'Wheel Of Fortune' Host Pat Sajak Undergoes Emergency Surgery; Vanna White Hosts Temporarily\n",
      "Wheel Of Fortune's Pat Sajak Undergoes 'Successful Emergency Surgery'\n",
      "Wheel Of Fortune's Pat Sajak Says 'Worst Has Passed' After Emergency Surgery Last Week\n",
      "Pat Sajak recovering from emergency surgery\n",
      "'Wheel of Fortune' fans can't believe all three contestants missed puzzle\n",
      "Wheel of Fortune's Pat Sajak Says the 'Worst Has Passed' Following Emergency Intestine Surgery\n",
      "ICYMI: The week in TV news for Oct. 13-19, 2019\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "get_recs_tfidf_first('U13740')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function only used the first article in the users reading history and as you can see, it **heavily emphasizes (and most certainly overestimates) the users interest in Wheel of Fortune topics**. Maybe we could do better in capturing our readers' interests by giving recommendations based on mutliple articles. Let's try it with three:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recs_tfidf_first_three(user_id):\n",
    "    used_articles = []\n",
    "    for i, article_id in enumerate(user_lookup.loc[user_id].history[0:3]):\n",
    "        used_articles.append(user_lookup.loc[user_id].history[i])\n",
    "    \n",
    "    \n",
    "    recs_for_articles = []\n",
    "    for article in used_articles:\n",
    "        recs_for_articles.append(recommended_articles(article, k=3))\n",
    "    \n",
    "    recs_for_articles_list = []\n",
    "    for array in recs_for_articles:\n",
    "        for article in array:\n",
    "            recs_for_articles_list.append(article)\n",
    "    \n",
    "    #return(recs_for_articles_list)\n",
    "   \n",
    "    article_titles = []\n",
    "    for article in recs_for_articles_list:\n",
    "        article_titles.append(mapping_title[article])\n",
    "    \n",
    "    \n",
    "    hits = 0\n",
    "    for article in recs_for_articles_list:\n",
    "        if article in user_lookup.loc[user_id].history:\n",
    "            hits += 1\n",
    "    \n",
    "    print(f'The first read articles of user {user_id} were: \\n {mapping_title[used_articles]}')\n",
    "    print('_______________________________________________________')\n",
    "    print(f'The suggested articles are:\\n')\n",
    "    for article in article_titles:\n",
    "        print(article) \n",
    "    print(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first read articles of user U13740 were: \n",
      " article_id\n",
      "N55189    'Wheel Of Fortune' Guest Delivers Hilarious, O...\n",
      "N42782    Three takeaways from Yankees' ALCS Game 5 vict...\n",
      "N34694    Rosie O'Donnell: Barbara Walters Isn't 'Up to ...\n",
      "Name: title, dtype: object\n",
      "_______________________________________________________\n",
      "The suggested articles are:\n",
      "\n",
      "Best Response Ever From a 'Wheel of Fortune' Contestant?\n",
      "Viral Wheel of Fortune Contestant and His Wife Clarify Hilarious 'Loveless Marriage' Intro\n",
      "'Wheel Of Fortune' Host Pat Sajak Recovers After Surgery\n",
      "Yankees stay alive with 4-1 win against Astros\n",
      "Three takeaways from Astros' Game 3 ALCS win over Yankees\n",
      "ALCS Game 6 Thread: Yankees at Astros\n",
      "Lawrence O'Donnell answers your impeachment questions\n",
      "3 Dead After Multi-Vehicle Crash Sparks 2-Acre Brush Fire in Santa Barbara\n",
      "Barbara Nicklaus honored with PGA's Distinguished Service Award\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "get_recs_tfidf_first_three('U13740')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the second article in user U13740's reading history being Yankess and baseball related, it ***could* be the case that we captured some long term interests of this person**. But this should rather be regarded as a coincidence: maybe ther person isn't even a yankees or a baseball fan and would hate us if we treated her like one by always suggesting Yankees or baseball related content!\n",
    "\n",
    "As we can see, **this approach is relatively twitchy with respect to relying on single (even if multiple single) articles**. We don't need to expect this method to be able to reasonably track user interests. The worse problem with news consumption is the fact that most people neither need nor want to read the same *news* again and again. Also, given that we have to do with news, it would be very important to **filter articles based on time and day of publication**. Unfortunately, this information is *not* provided in the MIcrosoft News Dataset, so we have to work out different strategies. \n",
    "\n",
    "In the following, we will pursue a matrix factorization approach, where we can make use of *all* the articles a user read together with all the articles all of the other users read.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative Filtering with Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following **collaborative filtering approach** takes into account all the readers and all the articles together. If we were to construct a matrix out of all the interactions and given that every user only read a tiny fraction of all  articles, we would get a very sparse matrix. The goal with a **matrix factorization technique** now is to 'learn' two embedding matrices with the repsective size of the numbers of readers/articles and an arbitrarily chosen (and thus tunable) size of latent factors. \n",
    "\n",
    "Thus, if we had 10 readers, 5 articles and were to assume we needed 3 latent factors (which could represent implicit, but substantive differences in our reader/article-base), **our method will calculate two matrices (a 10 by 3 for the readers and a 3 by 5 for the articles) whose scalar products yield a new matrix the size of our original one (10 x 5), which *approximates* the original matrix best**. This optimization problem is typically solved by stochastic gradient descent (although there are, of course, other possibilities) and from a once extremely sparse matrix (obviously, ervery single reader only reads/clicks a tiny fraction of the articles available to us), we get a densely populated table which now contains information on wether some reader might be more or less inclined to read certain articles. \n",
    "\n",
    "The approach might sound a bit dry and mathematic at first, but **with the embeddings we actually learn some lower dimensional representations of our readers/articles** and can hereby determine *resemblances in preferences*. If you ever wondered how amazon or google knew what you were interested in before you even searched for it: here you go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could do the matrix factorization manually using scikit-learns Truncated SVD for instance, but here, we use the **LightFM library**, which's main purpose actually is hybrid filtering, but **reduces to a matrix factorization when only supplied with user/article intrteractions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai = pd.read_csv('../../data/mind_small_train/small_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>user_int_id</th>\n",
       "      <th>article_int_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U13740</td>\n",
       "      <td>N55189</td>\n",
       "      <td>1810</td>\n",
       "      <td>24758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U13740</td>\n",
       "      <td>N42782</td>\n",
       "      <td>1810</td>\n",
       "      <td>17976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U13740</td>\n",
       "      <td>N34694</td>\n",
       "      <td>1810</td>\n",
       "      <td>13534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U13740</td>\n",
       "      <td>N45794</td>\n",
       "      <td>1810</td>\n",
       "      <td>19650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U13740</td>\n",
       "      <td>N18445</td>\n",
       "      <td>1810</td>\n",
       "      <td>4608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id article_id  user_int_id  article_int_id\n",
       "0  U13740     N55189         1810           24758\n",
       "1  U13740     N42782         1810           17976\n",
       "2  U13740     N34694         1810           13534\n",
       "3  U13740     N45794         1810           19650\n",
       "4  U13740     N18445         1810            4608"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uai.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have all the interactions and integers corresponding to the original IDs, which LightFM needs as Inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cf = Dataset()\n",
    "dataset_cf.fit(uai['user_int_id'], uai['article_int_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_array = uai.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions, weights = dataset_cf.build_interactions(\n",
    "    (ua[2], ua[3]) for ua in uai_array\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = cross_validation.random_train_test_split(\n",
    "    interactions, test_percentage=0.5, \n",
    "    random_state=np.random.RandomState(42)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these inputs, we can now build a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'warp'\n",
    "no_components = 20\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cf = LightFM(no_components=no_components, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x7fc3334ea978>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cf = LightFM(no_components=no_components, loss=loss)\n",
    "model_cf.fit(train, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC Score is in training/validation:                  0.98623747  /  0.906224\n",
      "The mean precision at k Score in training/validation is:  0.058934964  /  0.03676688\n",
      "The mean reciprocal rank in training/validation is:       0.19215415  /  0.12714417\n",
      "_________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(model_cf, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so this doesn't look too bad (the AUC score actually looks super high), although our model seems to be overfitting quite a bit. In order to later compare the matrix factorization approach with other deep learning models, we will **test our ranking scores by ranking a known postive interaction under 99 known negative ones**. We have the data prepared already so we only have to load it and write an evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_result = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id,article_id,user_int_id,article_int_id\n",
      "\n",
      "U13740,N31801,1810,11956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_filename = \"../../data/mind_small_train/small_test.csv\"\n",
    "test_positives = []\n",
    "\n",
    "with open(test_filename, \"r\") as f:\n",
    "    header = f.readline()\n",
    "    print(header)\n",
    "    line = f.readline()\n",
    "    print(line)\n",
    "    while line != None and line != \"\":\n",
    "        line_list = line.split(\",\")\n",
    "        #print(line_list)\n",
    "        user, article = int(line_list[2]), int(line_list[3])\n",
    "        #print(user, article)                                            \n",
    "        test_positives.append([user, article])\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_neg_filename = \"../../data/mind_small_train/small_test_negatives.tsv\"\n",
    "test_negatives = []\n",
    "\n",
    "with open(test_neg_filename, \"r\") as f:\n",
    "    line = f.readline()\n",
    "    while line != None and line != \"\":\n",
    "        line_list = line.split(\"\\t\")\n",
    "        #print(line_list)\n",
    "        negatives = []\n",
    "        for neg in line_list[1: ]:\n",
    "            negatives.append(int(neg))\n",
    "        test_negatives.append(negatives)\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_rating(idx, model):\n",
    "    user = test_positives[idx][0]\n",
    "    pos_item = test_positives[idx][1]\n",
    "    items = test_negatives[idx]\n",
    "    items.append(pos_item)\n",
    "    \n",
    "    # Get prediction score\n",
    "    map_item_score = {}\n",
    "    user_array = np.full(len(items), user, dtype = 'int32')\n",
    "    predictions = model.predict(user_array, np.array(items))\n",
    "    for i in range(len(items)):\n",
    "        item = items[i]\n",
    "        map_item_score[item] = predictions[i]\n",
    "    \n",
    "    items.pop()\n",
    "    \n",
    "    # Evaluate top rank list\n",
    "    ranklist = heapq.nlargest(K, map_item_score, key=map_item_score.get)\n",
    "    \n",
    "    if pos_item in ranklist:\n",
    "        hr = 1\n",
    "        i = ranklist.index(pos_item)\n",
    "        ndcg = np.log(2) / np.log(i+2)\n",
    "        rr = 1/(i+1)\n",
    "    else:\n",
    "        hr = 0\n",
    "        ndcg = 0\n",
    "        rr = 0\n",
    "   \n",
    "    return (hr, ndcg, rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits, ndcgs, rrs = [], [], []\n",
    "for idx in range(len(test_positives)):\n",
    "    hr, ndcg, rr = eval_one_rating(idx, model_cf)\n",
    "    hits.append(hr)\n",
    "    ndcgs.append(ndcg)\n",
    "    rrs.append(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = np.array(hits).mean()\n",
    "mrr = np.array(rrs).mean()\n",
    "ndcg = np.array(ndcgs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.057148300041675176, 0.21417640314079628)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr, mrr, ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so this will be a good baseline for our upcoming deep learning techniques. Now let's also use LightFM's hybrid capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Hybrid Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can include some **additional information concerning users and/or articles** when using a factorization machine like LighFM. Actually, that's what these algortihms are made for since we can already see that up until now, **we can't really make any recommendations for new users or articles**. Obviously, this **cold start problem** is a big issue in *news* recommendation as well! We won't tackle this problem at this point though, because we only want to demonstrate how to useLightFM as a hybrid. Also, the hybridization won't affect our customized testing drill, which only evaluates rankings for unlearned interactions of already learned users and articles. Later on, we will also show how the cold start problem could be tackled using recurrent neural networks.\n",
    "\n",
    "Right now, let's just imagine we wanted to exploit the available information on news categories (potentially, we could also make up some differentiating features for users, e.g. taking the extent of their reading history into account)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(\"../../data/mind_small_train/news_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_categories = news.category.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N55528</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N19639</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id   category      subcategory  \\\n",
       "0     N55528  lifestyle  lifestyleroyals   \n",
       "1     N19639     health       weightloss   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1                      50 Worst Habits For Belly Fat   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Shop the notebooks, jackets, and more that the...   \n",
       "1  These seemingly harmless habits are holding yo...   \n",
       "\n",
       "                                             url  \\\n",
       "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "\n",
       "                                      title_entities  \\\n",
       "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "\n",
       "                                   abstract_entities  \n",
       "0                                                 []  \n",
       "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lifestyle',\n",
       " 'health',\n",
       " 'news',\n",
       " 'sports',\n",
       " 'weather',\n",
       " 'entertainment',\n",
       " 'autos',\n",
       " 'travel',\n",
       " 'foodanddrink',\n",
       " 'tv',\n",
       " 'finance',\n",
       " 'movies',\n",
       " 'video',\n",
       " 'music',\n",
       " 'kids',\n",
       " 'middleeast',\n",
       " 'northamerica']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_cat_dict = {}\n",
    "for row in news.values:\n",
    "    art, cat = row[0], row[1]\n",
    "    article_cat_dict[art] = cat\n",
    "    \n",
    "for art in ['N2325787', 'N117002']:\n",
    "    article_cat_dict[art] = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_categories.append(\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_categories = [article_cat_dict[art] for art in uai.article_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_hybrid = Dataset()\n",
    "dataset_hybrid.fit(uai['user_int_id'], \n",
    "                   uai['article_int_id'],\n",
    "                   item_features=news_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = dataset_hybrid.build_item_features(\n",
    "    (art_id, [art_category]) for art_id, art_category \n",
    "    in zip(uai.article_int_id, article_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_hybrid, weights_hybrid = dataset_hybrid.build_interactions(\n",
    "    (ua[2], ua[3]) for ua in uai_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hybrid, test_hybrid = cross_validation.random_train_test_split(\n",
    "    interactions_hybrid, test_percentage=0.5,\n",
    "    random_state=np.random.RandomState(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightfm.lightfm.LightFM at 0x7fc345e82c88>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_hybrid = LightFM(no_components=no_components, \n",
    "                       loss=loss,\n",
    "                       item_alpha=0.0001)\n",
    "\n",
    "model_hybrid.fit(train_hybrid, \n",
    "                 item_features=item_features,\n",
    "                 epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC Score is in training/validation:                  0.947942  /  0.80881274\n",
      "The mean precision at k Score in training/validation is:  0.047876682  /  0.022506196\n",
      "The mean reciprocal rank in training/validation is:       0.19671887  /  0.090290435\n",
      "_________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "result_hybrid = evaluate(model_hybrid, train_hybrid, test_hybrid, \n",
    "                         hybrid=True, features=item_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, our hybrid model is overfitting the data even more than the one based solely on interactions. We could **experiment with a more severe regularization via the item alpha** for instance, but right now, we want to go ahead and pursue a neural collaborative filtering approach in the next notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
