{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network-Based Collaborative Filtering\n",
    "\n",
    "In this notebook we will implement a collaborative filtering model based on a Deep Neural Network (DNN). In contrast to classic matrix factorization, where an inner product of the latent features is calculated, **the NCF model is able to learn an arbitrary function to encapsulate non-linear user-item-interactions** utilizing a multi-layer perceptron architecture. The model is also able to learn different weights for the different latent factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (Embedding, Input, Dense, Reshape, \n",
    "                                     Flatten, Dropout)\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.metrics import MeanSquaredError, Precision, AUC\n",
    "\n",
    "from NCFHelper import eval_one_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose dataset type and size\n",
    "dataset_type = \"train\"\n",
    "dataset_size = \"small\"\n",
    "\n",
    "data_path = f\"../../data/mind_{dataset_size}_{dataset_type}/\"\n",
    "train_filename = data_path + f\"{dataset_size}_train.csv\"\n",
    "test_filename = data_path + f\"{dataset_size}_test.csv\"\n",
    "test_negatives_fn = data_path + f\"{dataset_size}_test_negatives.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users, num_articles = 0, 0\n",
    "with open(train_filename, \"r\") as f:\n",
    "    header = f.readline()\n",
    "    # print(header)\n",
    "    line = f.readline()\n",
    "    # print(line)\n",
    "    while line != None and line != \"\":\n",
    "        line_list = line.split(\",\")\n",
    "        u, i = int(line_list[2]), int(line_list[3])\n",
    "        num_users = max(num_users, u)\n",
    "        num_articles = max(num_articles, i)\n",
    "        line = f.readline()\n",
    "\n",
    "num_users += 1\n",
    "num_articles += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users, num_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sp.dok_matrix((num_users, num_articles), dtype=np.float32)\n",
    "\n",
    "with open(train_filename, \"r\") as f:\n",
    "    header = f.readline()\n",
    "    # print(header)\n",
    "    line = f.readline()\n",
    "    # print(line)\n",
    "    while line != None and line != \"\":\n",
    "        line_list = line.split(\",\")\n",
    "        user, article = int(line_list[2]), int(line_list[3])\n",
    "        train[user, article] = 1.0\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_ratio = 4\n",
    "user_train, article_train, labels_train = [],[],[]\n",
    "for (u, i) in train.keys():\n",
    "    # positive instance\n",
    "    user_train.append(u)\n",
    "    article_train.append(i)\n",
    "    labels_train.append(1)\n",
    "    # negative instances\n",
    "    for t in range(np_ratio):\n",
    "        j = np.random.randint(num_articles)\n",
    "        while (u, j) in train.keys():\n",
    "            j = np.random.randint(num_articles)\n",
    "        user_train.append(u)\n",
    "        article_train.append(j)\n",
    "        labels_train.append(0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(user_train), len(article_train), len(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_positives = []\n",
    "test_negatives = []\n",
    "with open(test_negatives_fn, \"r\") as f:\n",
    "    line = f.readline()\n",
    "    while line != None and line != \"\":\n",
    "        line_list = line.split(\"\\t\")\n",
    "        # print(line_list)\n",
    "        ua = line_list[0].strip(\"()\").split(\",\")\n",
    "        user, article = int(ua[0]), int(ua[1])\n",
    "        test_positives.append([user, article])\n",
    "        # print(user)\n",
    "        # print(article)\n",
    "        negatives = []\n",
    "        for neg in line_list[1: ]:\n",
    "            negatives.append(int(neg))\n",
    "        test_negatives.append(negatives)\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_positives), len(test_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua_train, ua_val, label_train, label_val = train_test_split(np.column_stack((user_train, article_train)),\n",
    "                                                            labels_train, random_state=SEED, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_train, article_train = ua_train[:, 0], ua_train[:, 1]\n",
    "user_val, article_val = ua_val[:, 0], ua_val[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [64, 32, 16, 8]\n",
    "reg_layers = [1e-4, 1e-4, 1e-4, 1e-4]\n",
    "num_layer = len(layers)\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_size=256\n",
    "loss = 'binary_crossentropy'\n",
    "SEED = 420\n",
    "dropout = True\n",
    "dropout_rates = [0, 0.2, 0.2, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "article_input = Input(shape=(1,), dtype='int32', name='article_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_Embedding_User = Embedding(input_dim=num_users, output_dim=layers[0]//2, \n",
    "                               embeddings_regularizer=l2(reg_layers[0]),\n",
    "                               name='user_embedding', input_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_Embedding_Article = Embedding(input_dim=num_articles, output_dim=layers[0]//2, \n",
    "                                  embeddings_regularizer=l2(reg_layers[0]),\n",
    "                                  name='article_embedding', input_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "article_latent = Flatten()(MLP_Embedding_Article(article_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = Concatenate(axis=-1)([user_latent, article_latent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(1, num_layer):\n",
    "    layer = Dense(layers[idx], activation='relu', \n",
    "                  kernel_regularizer=l2(reg_layers[idx]), name=f'layer{idx}')\n",
    "    if dropout:\n",
    "        drop = Dropout(dropout_rates[idx-1], seed=SEED)\n",
    "    vector = drop(layer(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name='prediction')(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[user_input, article_input], outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=learning_rate), loss=loss,\n",
    "              metrics=[Precision(), AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua_train, ua_val, label_train, label_val = train_test_split(np.column_stack((user_train, article_train)),\n",
    "                                                            labels_train, random_state=SEED, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_train, article_train = ua_train[:, 0], ua_train[:, 1]\n",
    "user_val, article_val = ua_val[:, 0], ua_val[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit([np.array(user_train), np.array(article_train)], #input\n",
    "                 np.array(label_train), # labels\n",
    "                 validation_data=([np.array(user_val), np.array(article_val)], np.array(label_val)),\n",
    "                 batch_size=batch_size, \n",
    "                 epochs=epochs, \n",
    "                 verbose=1, \n",
    "                 shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "iterations = len(test_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits, ndcgs, rrs = [], [], []\n",
    "for idx in range(iterations):\n",
    "    print(round((idx/iterations)*100, 2), end=\"\\r\")\n",
    "    hr, ndcg, rr = eval_one_rating(idx, model, test_positives, test_negatives, K)\n",
    "    hits.append(hr)\n",
    "    ndcgs.append(ndcg)\n",
    "    rrs.append(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = np.array(hits).mean()\n",
    "mrr = np.array(rrs).mean()\n",
    "ndcg = np.array(ndcgs).mean()\n",
    "\n",
    "print(\"Hit ratio:            \", hr)\n",
    "print(\"Mean reciprocal rank: \", mrr)\n",
    "print(f\"NDCG@{K}:            \", ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out_file = f\"trained-models/{dataset_size}_MLP_{layers}\"\n",
    "model_out_file = f's3://{bucket}/ncf-large/ncf-model-large-'\n",
    "\n",
    "date_time = datetime.now.strftime(\"%m/%d/%Y_%H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_out_file+date_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ncf model can also be extended to incorporate information about users and articles (e.g. categories, titles, ..) by extending the input layers. In this sense it would be a hybrid model of collaborative and content-based filtering. \n",
    "\n",
    "Another thing that has been done is to combine the matrix factorization approach with the DNN approach. This will be done in the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Matrix Factorization (NeuMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "BATCH_SIZE = 256\n",
    "NUM_FACTORS = 8\n",
    "LAYERS = [64,32,16,8]\n",
    "REG_MF = 0\n",
    "REG_LAYERS = [0,0,0,0]\n",
    "REGS = [0, 0]\n",
    "NUM_NEG = 4\n",
    "LR = 0.001\n",
    "LEARNER = \"adam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topK = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYER = len(LAYERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "article_input = Input(shape=(1,), dtype='int32', name='article_input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User and Article Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_Embedding_User = Embedding(input_dim=num_users, \n",
    "                              output_dim=NUM_FACTORS, \n",
    "                              name='mf_user_embedding',\n",
    "                              input_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MF_Embedding_Article = Embedding(input_dim=num_articles, \n",
    "                                 output_dim=NUM_FACTORS, \n",
    "                                 name = 'mf_article_embedding',\n",
    "                                 input_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mulit-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_Embedding_User = Embedding(input_dim=num_users, output_dim=LAYERS[0]//2, \n",
    "                               name='mlp_user_embedding', input_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_Embedding_Article = Embedding(input_dim=num_articles, output_dim=LAYERS[0]//2, \n",
    "                               name='mlp_article_embedding', input_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MF and MLP Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "mf_article_latent = Flatten()(MF_Embedding_Article(article_input))\n",
    "\n",
    "mf_vector = Multiply()([mf_user_latent, mf_article_latent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "mlp_article_latent = Flatten()(MLP_Embedding_Article(article_input))\n",
    "\n",
    "mlp_vector = Concatenate(axis=-1)([mlp_user_latent, mlp_article_latent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(1, NUM_LAYER):\n",
    "    layer = Dense(LAYERS[idx], activation='relu', name='layer%d' %idx)\n",
    "    mlp_vector = layer(mlp_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_vector = Concatenate()([mf_vector, mlp_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = Dense(1, activation='sigmoid', \n",
    "                   kernel_initializer='lecun_uniform', \n",
    "                   name = 'prediction')(predict_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([user_input, article_input], prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=LR), loss='binary_crossentropy',\n",
    "             metrics=[MeanSquaredError(), Precision(), AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_neu = model.fit([np.array(user_train), np.array(article_train)], #input\n",
    "                     np.array(label_train), # labels \n",
    "                     validation_data=([np.array(user_val), np.array(article_val)], np.array(label_val))\n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=1, \n",
    "                     verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10\n",
    "iterations = len(test_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits, ndcgs, rrs = [], [], []\n",
    "for idx in range(iterations):\n",
    "    print(round((idx/iterations)*100, 2), end=\"\\r\")\n",
    "    hr, ndcg, rr = eval_one_rating(idx, model_neu, test_positives, test_negatives, K)\n",
    "    hits.append(hr)\n",
    "    ndcgs.append(ndcg)\n",
    "    rrs.append(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = np.array(hits).mean()\n",
    "mrr = np.array(rrs).mean()\n",
    "ndcg = np.array(ndcgs).mean()\n",
    "\n",
    "print(\"Hit ratio:            \", hr)\n",
    "print(\"Mean reciprocal rank: \", mrr)\n",
    "print(f\"NDCG@{topK}:         \", ndcg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
