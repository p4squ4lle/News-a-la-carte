{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install smart_open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from smart_open import smart_open\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Input, Dense, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket='reco-mind-data/mind_large_train'\n",
    "train = 'large_train_npratio4.tsv'\n",
    "#test_negatives = 'large_test_negatives.tsv'\n",
    "\n",
    "data_location_train = 's3://{}/{}'.format(bucket, train)\n",
    "#data_location_test = 's3://{}/{}'.format(bucket, test)\n",
    "#data_location_test_negatives = 's3://{}/{}'.format(bucket, test_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_train, article_train, labels_train = [],[],[]\n",
    "with smart_open(data_location_train, \"r\") as f:\n",
    "    line = f.readline()\n",
    "    while line != None and line != \"\":\n",
    "        line_list = line.split(\"\\t\")\n",
    "        user, article, label = line_list[0], line_list[1], line_list[2]\n",
    "        user_train.append(int(user))\n",
    "        article_train.append(int(article))\n",
    "        labels_train.append(int(label))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [64, 32, 16, 8]\n",
    "reg_layers = [0, 0, 0, 0]\n",
    "num_layer = len(layers)\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "loss = 'binary_crossentropy'\n",
    "SEED = 420\n",
    "dropout = True\n",
    "dropout_rates = [0, 0.2, 0.2, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua_train, ua_val, label_train, label_val = train_test_split(np.column_stack((user_train, article_train)),\n",
    "                                                            labels_train, random_state=SEED, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_train, article_train = ua_train[:, 0], ua_train[:, 1]\n",
    "user_val, article_val = ua_val[:, 0], ua_val[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = Input(shape=(1,), dtype='int32', name='user_input')\n",
    "article_input = Input(shape=(1,), dtype='int32', name='article_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_Embedding_User = Embedding(input_dim=num_users, output_dim=layers[0]//2, \n",
    "                               embeddings_regularizer=l2(reg_layers[0]),\n",
    "                               name='user_embedding', input_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_Embedding_Article = Embedding(input_dim=num_articles, output_dim=layers[0]//2, \n",
    "                                  embeddings_regularizer=l2(reg_layers[0]),\n",
    "                                  name='article_embedding', input_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "article_latent = Flatten()(MLP_Embedding_Article(article_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = Concatenate(axis=-1)([user_latent, article_latent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(1, num_layer):\n",
    "    layer = Dense(layers[idx], activation='relu', \n",
    "                  kernel_regularizer=l2(reg_layers[idx]), name=f'layer{idx}')\n",
    "    if dropout:\n",
    "        drop = Dropout(dropout_rates[idx-1], seed=SEED)\n",
    "    vector = drop(layer(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name='prediction')(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[user_input, article_input], outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=learning_rate), loss=loss,\n",
    "              metrics=[Precision(), AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua_train, ua_val, label_train, label_val = train_test_split(np.column_stack((user_train, article_train)),\n",
    "                                                            labels_train, random_state=SEED, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_train, article_train = ua_train[:, 0], ua_train[:, 1]\n",
    "user_val, article_val = ua_val[:, 0], ua_val[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit([np.array(user_train), np.array(article_train)],\n",
    "                 np.array(label_train),\n",
    "                 validation_data=([np.array(user_val), np.array(article_val)], np.array(label_val)),\n",
    "                 batch_size=batch_size, \n",
    "                 epochs=epochs, \n",
    "                 verbose=1, \n",
    "                 shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_out_file = f's3://{bucket}/ncf-large/history-'\n",
    "model_out_file = f's3://{bucket}/ncf-large/ncf-model-large-'\n",
    "\n",
    "date_time = datetime.now.strftime(\"%m/%d/%Y_%H:%M:%S\")\n",
    "\n",
    "with smart_open(model_out_file+date_time, \"wb\"):\n",
    "    model.save(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_out_file = f's3://{bucket}/ncf-large/history-'\n",
    "with smart_open(history_out_file+date_time, 'wb') as file:\n",
    "        pickle.dump(history.history, filepi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
