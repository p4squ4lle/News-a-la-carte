{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Cleaning of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from progressbar import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors = pd.read_csv(\"../../data/mind_small_train/behaviors.tsv\", sep='\\t', header=None)\n",
    "news = pd.read_csv(\"../../data/mind_small_train/news.tsv\", sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first give our datasets some proper column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors = behaviors.rename(columns={0:'impression_id', 1 : 'user_id', 2 : 'time', 3:'history', 4 : 'impressions'})\n",
    "news = news.rename(columns={0:'article_id', 1:'category', 2:'subcategory', 3:'title', 4:'abstract', 5:'url', 6:'title_entities', 7:'abstract_entities'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N55528</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N19639</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N61837</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJgNsz.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id   category      subcategory  \\\n",
       "0     N55528  lifestyle  lifestyleroyals   \n",
       "1     N19639     health       weightloss   \n",
       "2     N61837       news        newsworld   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1                      50 Worst Habits For Belly Fat   \n",
       "2  The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Shop the notebooks, jackets, and more that the...   \n",
       "1  These seemingly harmless habits are holding yo...   \n",
       "2  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
       "\n",
       "                                             url  \\\n",
       "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "2  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
       "\n",
       "                                      title_entities  \\\n",
       "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "2                                                 []   \n",
       "\n",
       "                                   abstract_entities  \n",
       "0                                                 []  \n",
       "1  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  \n",
       "2  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51282, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have more than 100,000 news articles in our *news* dataset with information concerning the **news category, subcategory, it's title, abstract and even some entitiy embeddings** (most of the urls don't work anymore so we don't have access to the full bodies). Let's check whether these are all unique articles or if we also have some duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique news articles:  50434\n",
      "Number of duplicates:              848\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique news articles: \", news.title.nunique())\n",
    "print(\"Number of duplicates:             \", news.shape[0] - news.title.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, there are **news articles with multiple IDs**. We don't just want to drop them, because this would result in a loss of useful information concerning the click behaviors and reading histories in our *behaviors* dataset, which looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>time</th>\n",
       "      <th>history</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U13740</td>\n",
       "      <td>11/11/2019 9:05:58 AM</td>\n",
       "      <td>N55189 N42782 N34694 N45794 N18445 N63302 N104...</td>\n",
       "      <td>N55689-1 N35729-0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U91836</td>\n",
       "      <td>11/12/2019 6:11:30 PM</td>\n",
       "      <td>N31739 N6072 N63045 N23979 N35656 N43353 N8129...</td>\n",
       "      <td>N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U73700</td>\n",
       "      <td>11/14/2019 7:01:48 AM</td>\n",
       "      <td>N10732 N25792 N7563 N21087 N41087 N5445 N60384...</td>\n",
       "      <td>N50014-0 N23877-0 N35389-0 N49712-0 N16844-0 N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impression_id user_id                   time  \\\n",
       "0              1  U13740  11/11/2019 9:05:58 AM   \n",
       "1              2  U91836  11/12/2019 6:11:30 PM   \n",
       "2              3  U73700  11/14/2019 7:01:48 AM   \n",
       "\n",
       "                                             history  \\\n",
       "0  N55189 N42782 N34694 N45794 N18445 N63302 N104...   \n",
       "1  N31739 N6072 N63045 N23979 N35656 N43353 N8129...   \n",
       "2  N10732 N25792 N7563 N21087 N41087 N5445 N60384...   \n",
       "\n",
       "                                         impressions  \n",
       "0                                  N55689-1 N35729-0  \n",
       "1  N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...  \n",
       "2  N50014-0 N23877-0 N35389-0 N49712-0 N16844-0 N...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first prepare this dataset before we get back to handling the duplicate news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the *behaviors* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156965, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have more than two million online sessions on msn with information concerning **user ID, date and daytime, the click history, and the recommended articles and user behavior** (ending on -1 = clicked) for the respective session. Let's see how many unique users there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50000 individual users in our dataset.\n",
      "The average number of sessions is: 3.14\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(behaviors.user_id.unique())} individual users in our dataset.')\n",
    "print(f'The average number of sessions is: {behaviors.shape[0] / len(behaviors.user_id.unique()):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since we have to work with the click history, which was recorded before the sessions and is the same for all the sessions, as these two random samples show:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_id = behaviors[behaviors.user_id == 'U13740']\n",
    "len(behaviors_id.history), len(behaviors_id.history.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_id = behaviors[behaviors.user_id == 'U73700']\n",
    "len(behaviors_id.history), len(behaviors_id.history.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to include **only users with at least five articles read** in their history. So we need to drop ahistorical users as well as users with too few articles read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "impression_id       0\n",
       "user_id             0\n",
       "time                0\n",
       "history          3238\n",
       "impressions         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153727, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors['length_history'] = behaviors.history.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors['length_history'] = behaviors.length_history.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors = behaviors[behaviors['length_history'] >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5      6101\n",
       "6      5591\n",
       "7      5341\n",
       "8      4636\n",
       "9      4270\n",
       "       ... \n",
       "198       5\n",
       "293       4\n",
       "283       2\n",
       "193       1\n",
       "301       1\n",
       "Name: length_history, Length: 251, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors.length_history.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136120"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With dropping those users, the numbers for our *behaviors* dataset now look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40331 individual users in our dataset.\n",
      "The average number of sessions is: 3.38\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(behaviors.user_id.unique())} individual users in our dataset.')\n",
    "print(f'The average number of sessions is: {behaviors.shape[0] / len(behaviors.user_id.unique()):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Droppping duplicate article IDs in *news* and remapping them in *behaviors*\n",
    "Now that we have set up our *behaviors* dataset with respect to users, let's get back to the duplicate news articles. With different IDs for the de facto same articles we would not be able to track similarities among users sufficiently. In the following, we will **replace every redundant article-ID with the first ID for the respective article**. In order to this, we first create a subset with all the duplicates in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplis_title = news[news.duplicated(subset=\"title\", keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35832</th>\n",
       "      <td>N34049</td>\n",
       "      <td>sports</td>\n",
       "      <td>football_nfl</td>\n",
       "      <td>'A game-changer': Titans' expansion project wi...</td>\n",
       "      <td>The Titans have begun construction on a 60,000...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/BBWHHH8.html</td>\n",
       "      <td>[{\"Label\": \"Tennessee Titans\", \"Type\": \"O\", \"W...</td>\n",
       "      <td>[{\"Label\": \"Tennessee Titans\", \"Type\": \"O\", \"W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33999</th>\n",
       "      <td>N56680</td>\n",
       "      <td>sports</td>\n",
       "      <td>football_nfl</td>\n",
       "      <td>'A game-changer': Titans' expansion project wi...</td>\n",
       "      <td>The Titans have begun construction on a 60,000...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/BBWH8ZY.html</td>\n",
       "      <td>[{\"Label\": \"Tennessee Titans\", \"Type\": \"O\", \"W...</td>\n",
       "      <td>[{\"Label\": \"Tennessee Titans\", \"Type\": \"O\", \"W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45242</th>\n",
       "      <td>N16531</td>\n",
       "      <td>news</td>\n",
       "      <td>newscrime</td>\n",
       "      <td>'Baby Trump' balloon slashed at Alabama appear...</td>\n",
       "      <td>TUSCALOOSA, Ala. (AP) Organizers say a man sla...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/BBWwk8L.html</td>\n",
       "      <td>[{\"Label\": \"Donald Trump baby balloon\", \"Type\"...</td>\n",
       "      <td>[{\"Label\": \"Donald Trump baby balloon\", \"Type\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id category   subcategory  \\\n",
       "35832     N34049   sports  football_nfl   \n",
       "33999     N56680   sports  football_nfl   \n",
       "45242     N16531     news     newscrime   \n",
       "\n",
       "                                                   title  \\\n",
       "35832  'A game-changer': Titans' expansion project wi...   \n",
       "33999  'A game-changer': Titans' expansion project wi...   \n",
       "45242  'Baby Trump' balloon slashed at Alabama appear...   \n",
       "\n",
       "                                                abstract  \\\n",
       "35832  The Titans have begun construction on a 60,000...   \n",
       "33999  The Titans have begun construction on a 60,000...   \n",
       "45242  TUSCALOOSA, Ala. (AP) Organizers say a man sla...   \n",
       "\n",
       "                                                 url  \\\n",
       "35832  https://assets.msn.com/labs/mind/BBWHHH8.html   \n",
       "33999  https://assets.msn.com/labs/mind/BBWH8ZY.html   \n",
       "45242  https://assets.msn.com/labs/mind/BBWwk8L.html   \n",
       "\n",
       "                                          title_entities  \\\n",
       "35832  [{\"Label\": \"Tennessee Titans\", \"Type\": \"O\", \"W...   \n",
       "33999  [{\"Label\": \"Tennessee Titans\", \"Type\": \"O\", \"W...   \n",
       "45242  [{\"Label\": \"Donald Trump baby balloon\", \"Type\"...   \n",
       "\n",
       "                                       abstract_entities  \n",
       "35832  [{\"Label\": \"Tennessee Titans\", \"Type\": \"O\", \"W...  \n",
       "33999  [{\"Label\": \"Tennessee Titans\", \"Type\": \"O\", \"W...  \n",
       "45242  [{\"Label\": \"Donald Trump baby balloon\", \"Type\"...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplis_title.sort_values(by=\"title\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_set = duplis_title['title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = []\n",
    "for title in title_set:\n",
    "    x = duplis_title[duplis_title['title']==title]['article_id'].to_list()\n",
    "    article_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['N61864', 'N47020'],\n",
       " ['N59709', 'N13882', 'N57732', 'N56582'],\n",
       " ['N6632', 'N39995'],\n",
       " ['N14042', 'N21933'],\n",
       " ['N37736', 'N22941', 'N60979']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a **list which contains article IDs for every article which has multiple IDs** in our original dataset. With this list, we can generate a dictionary called articleID_dict, which maps all the redundant IDs (keys) to a single ID (value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "articleID_dict = {}\n",
    "articles_to_change = []\n",
    "for article in article_list:\n",
    "    value = article[0]\n",
    "    keys = article [1:]\n",
    "    for k in keys:\n",
    "        articleID_dict[k] = value\n",
    "        articles_to_change.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a copy of the original behaviors dataframe and make it to a numpy array, so that we can **loop through all the redundant IDs in the *behaviors* dataset and homogenize them** according to our dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav = behaviors.copy()\n",
    "behav = behav.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = ProgressBar()\n",
    "userIDs_hist_changes = []\n",
    "userIDs_impr_changes = []\n",
    "users_to_change = []\n",
    "articles_to_change_set = set(articles_to_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "for idx in pbar(range(behav.shape[0])):\n",
    "    user_row = behav[idx]\n",
    "    hist_flag = False\n",
    "    hist = user_row[3]\n",
    "    hist_list = hist.split()\n",
    "    hist_set = set(hist_list)\n",
    "    hist_inter = hist_set & articles_to_change_set\n",
    "    for art in hist_inter:\n",
    "        hist_flag = True\n",
    "        users_to_change.append(idx)\n",
    "        userIDs_hist_changes.append(user_row[1])\n",
    "        hist = hist.replace(art, articleID_dict[art])\n",
    "    if hist_flag:\n",
    "        behav[idx][3] = hist\n",
    "    impression_flag = False\n",
    "    impressions = user_row[4]\n",
    "    impression_list = [l[:-2] for l in impressions.split()]\n",
    "    impression_set = set(impression_list)\n",
    "    for art in (impression_set & articles_to_change_set):\n",
    "        impression_flag = True\n",
    "        userIDs_impr_changes.append(user_row[1])\n",
    "        impressions = impressions.replace(art, articleID_dict[art])\n",
    "    if impression_flag:        \n",
    "        behav[idx][4] = impressions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's **check wether our method worked**. For this task, we **construct a list containing all articles in our array and compare it to the articles_to_change list**, by making them to sets and calculating their intersection, which should be empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = []\n",
    "for row in behav:\n",
    "    hist_list=row[3].split(' ')\n",
    "    for article in hist_list:\n",
    "        all_articles.append(article)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(all_articles) & articles_to_change_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, our method worked! Now let's make a **new dataframe out of our processed behavioral data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_new = pd.DataFrame(behav, columns= behaviors.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a new one for the information on **news articles without duplicates**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_new = news.drop_duplicates(subset=\"title\", keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving processed datasets\n",
    "Now we want to save the processed data and write it to csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_new.to_csv(\"../../data/mind_small_train/behaviors_processed_small.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_new.to_csv(\"../../data/mind_small_train/news_processed_small.csv\", \n",
    "                    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for collaborative filtering approaches\n",
    "For the deployment of recommender systems which use collaborative filtering (CF) techniques, **user-article interactions play a pivotal role**. Because CF is of great importance to understand modern day recommender systems in general, we too want to construct and discuss different versions of this approach. In order to do this, it is useful to further process our data with repsect to user-article interactions. \n",
    "\n",
    "Because we **only work with the click history** when deploying CF methods, we only need one session per user:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_cf = behaviors_new.drop_duplicates(subset='user_id').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_cf.shape[0] == behaviors_new.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>time</th>\n",
       "      <th>history</th>\n",
       "      <th>impressions</th>\n",
       "      <th>length_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U13740</td>\n",
       "      <td>11/11/2019 9:05:58 AM</td>\n",
       "      <td>N55189 N42782 N34694 N45794 N18445 N63302 N104...</td>\n",
       "      <td>N55689-1 N35729-0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U91836</td>\n",
       "      <td>11/12/2019 6:11:30 PM</td>\n",
       "      <td>N31739 N6072 N63045 N23979 N35656 N43353 N8129...</td>\n",
       "      <td>N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U73700</td>\n",
       "      <td>11/14/2019 7:01:48 AM</td>\n",
       "      <td>N10732 N25792 N7563 N21087 N41087 N5445 N60384...</td>\n",
       "      <td>N50014-0 N23877-0 N35389-0 N49712-0 N16844-0 N...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>U34670</td>\n",
       "      <td>11/11/2019 5:28:05 AM</td>\n",
       "      <td>N45729 N2203 N871 N53880 N41375 N43142 N33013 ...</td>\n",
       "      <td>N35729-0 N33632-0 N49685-1 N27581-0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>U19739</td>\n",
       "      <td>11/11/2019 6:52:13 PM</td>\n",
       "      <td>N39074 N14343 N32607 N32320 N22007 N442 N19001...</td>\n",
       "      <td>N21119-1 N53696-0 N33619-1 N25722-0 N2869-0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  impression_id user_id                   time  \\\n",
       "0             1  U13740  11/11/2019 9:05:58 AM   \n",
       "1             2  U91836  11/12/2019 6:11:30 PM   \n",
       "2             3  U73700  11/14/2019 7:01:48 AM   \n",
       "3             4  U34670  11/11/2019 5:28:05 AM   \n",
       "4             6  U19739  11/11/2019 6:52:13 PM   \n",
       "\n",
       "                                             history  \\\n",
       "0  N55189 N42782 N34694 N45794 N18445 N63302 N104...   \n",
       "1  N31739 N6072 N63045 N23979 N35656 N43353 N8129...   \n",
       "2  N10732 N25792 N7563 N21087 N41087 N5445 N60384...   \n",
       "3  N45729 N2203 N871 N53880 N41375 N43142 N33013 ...   \n",
       "4  N39074 N14343 N32607 N32320 N22007 N442 N19001...   \n",
       "\n",
       "                                         impressions length_history  \n",
       "0                                  N55689-1 N35729-0              9  \n",
       "1  N20678-0 N39317-0 N58114-0 N20495-0 N42977-0 N...             82  \n",
       "2  N50014-0 N23877-0 N35389-0 N49712-0 N16844-0 N...             16  \n",
       "3                N35729-0 N33632-0 N49685-1 N27581-0             10  \n",
       "4        N21119-1 N53696-0 N33619-1 N25722-0 N2869-0             36  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_cf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to construct a numpy array out of this smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_cf = behaviors_cf.to_numpy(copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so that we can get **two arrays with user-article-interactions**. One for training and another one with the last article in history for testing purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_train, uai_test = [], []\n",
    "\n",
    "for row in behav_cf:\n",
    "    user = row[1]\n",
    "    hist = row[3].split(' ')\n",
    "    for art in hist[:-1]:\n",
    "        uai_train.append([user, art])\n",
    "    last_art = hist[-1]\n",
    "    uai_test.append([user, last_art])\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if that worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(behaviors.loc[0].history.split(' ')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['U13740', 'N55189'],\n",
       "  ['U13740', 'N42782'],\n",
       "  ['U13740', 'N34694'],\n",
       "  ['U13740', 'N45794'],\n",
       "  ['U13740', 'N18445'],\n",
       "  ['U13740', 'N63302'],\n",
       "  ['U13740', 'N10414'],\n",
       "  ['U13740', 'N19347'],\n",
       "  ['U91836', 'N31739'],\n",
       "  ['U91836', 'N6072'],\n",
       "  ['U91836', 'N63045'],\n",
       "  ['U91836', 'N23979'],\n",
       "  ['U91836', 'N35656'],\n",
       "  ['U91836', 'N43353'],\n",
       "  ['U91836', 'N8129']],\n",
       " ['U13740', 'N31801'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uai_train[0:15], uai_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good! Now we want to get some **extra user- and article integer IDs**, that we can later use for a **dictionary-of-keys-matrix**, which in turn will be **employed in a neural network**. For our train data, we can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_train_df = pd.DataFrame(uai_train, columns=['user_id', 'article_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_train_df['user_int_id'] = uai_train_df.user_id.astype('category').cat.codes\n",
    "uai_train_df['article_int_id'] = uai_train_df.article_id.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>user_int_id</th>\n",
       "      <th>article_int_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U13740</td>\n",
       "      <td>N55189</td>\n",
       "      <td>1810</td>\n",
       "      <td>24758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U13740</td>\n",
       "      <td>N42782</td>\n",
       "      <td>1810</td>\n",
       "      <td>17976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U13740</td>\n",
       "      <td>N34694</td>\n",
       "      <td>1810</td>\n",
       "      <td>13534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id article_id  user_int_id  article_int_id\n",
       "0  U13740     N55189         1810           24758\n",
       "1  U13740     N42782         1810           17976\n",
       "2  U13740     N34694         1810           13534"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uai_train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for our test data, we need to make sure that it contains only the articles which are also in the train data. At first, we need to find those articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_articles = [elem[1] for elem in uai_train]\n",
    "test_articles = [elem[1] for elem in uai_test]\n",
    "articles_to_drop = set(test_articles)-set(train_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this list, we're now able to reduce our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_test_red = [ele for ele in uai_test if ele[1] not in articles_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and can thus create a test dataframe without unknown articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_test_df = pd.DataFrame(uai_test_red, columns=['user_id', 'article_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then construct two dictionaries, which map the original user and article IDs to the integer IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_code_dict = pd.Series(uai_train_df.user_int_id.values,\n",
    "                           index=uai_train_df.user_id).to_dict()\n",
    "\n",
    "article_code_dict = pd.Series(uai_train_df.article_int_id.values,\n",
    "                              index=uai_train_df.article_id).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_test_df['user_int_id'] = [user_code_dict[user] for user in uai_test_df.user_id]\n",
    "uai_test_df['article_int_id'] = [article_code_dict[art] for art in uai_test_df.article_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how that worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>user_int_id</th>\n",
       "      <th>article_int_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U13740</td>\n",
       "      <td>N31801</td>\n",
       "      <td>1810</td>\n",
       "      <td>11956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1357</th>\n",
       "      <td>U21185</td>\n",
       "      <td>N31801</td>\n",
       "      <td>5395</td>\n",
       "      <td>11956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1379</th>\n",
       "      <td>U19379</td>\n",
       "      <td>N31801</td>\n",
       "      <td>4529</td>\n",
       "      <td>11956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2051</th>\n",
       "      <td>U4891</td>\n",
       "      <td>N31801</td>\n",
       "      <td>18423</td>\n",
       "      <td>11956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3876</th>\n",
       "      <td>U81689</td>\n",
       "      <td>N31801</td>\n",
       "      <td>34165</td>\n",
       "      <td>11956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id article_id  user_int_id  article_int_id\n",
       "0     U13740     N31801         1810           11956\n",
       "1357  U21185     N31801         5395           11956\n",
       "1379  U19379     N31801         4529           11956\n",
       "2051   U4891     N31801        18423           11956\n",
       "3876  U81689     N31801        34165           11956"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uai_test_df_example = uai_test_df[uai_test_df['article_id'] == 'N31801']\n",
    "uai_test_df_example.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's save those dataframes to csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_train_df.to_csv(\"../../data/mind_small_train/small_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_test_df.to_csv(\"../../data/mind_small_train/small_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actually **create the dok-matrix**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = \"../../data/mind_small_train/small_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40331, 32101)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users, num_articles = uai_train_df.user_id.nunique(), uai_train_df.article_id.nunique()\n",
    "num_users, num_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id,article_id,user_int_id,article_int_id\n",
      "\n",
      "U13740,N55189,1810,24758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_matrix = sp.dok_matrix((num_users, num_articles), dtype=np.float32)\n",
    "\n",
    "with open(train_filename, \"r\") as f:\n",
    "    header = f.readline()\n",
    "    line = f.readline()\n",
    "    print(header)\n",
    "    print(line)\n",
    "    while line != None and line != \"\":\n",
    "        line_list = line.split(\",\")\n",
    "        user, article = int(line_list[2]), int(line_list[3])\n",
    "        train_matrix[user, article] = 1.0\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, when we want to evaluate our recommender systems, we need to **compare the ranking for the known -- but not learned -- interactions with known non-interactions**, so that we can tell how useful our recommendation is: the higher the ranking of the test interaction, the better our model! In order to do this, we want to **extract 99 non read articles for every user in our test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_interactions = list(zip(uai_test_df.user_int_id, uai_test_df.article_int_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_negatives = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_interactions = []\n",
    "for u, i in test_interactions:\n",
    "    negatives = []\n",
    "    for t in range(num_negatives):\n",
    "        j = np.random.randint(num_articles)\n",
    "        while (u, j) in train_matrix.keys():\n",
    "            j = np.random.randint(num_articles)\n",
    "        negatives.append(j)\n",
    "    negative_interactions.append(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39846, 39846, 99)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_interactions), len(negative_interactions), len(negative_interactions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to write our one positive interaction along with the randomly generated non interactions into a csv file and we're done with preprocessing the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = negative_interactions[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_interactions)):\n",
    "    output[i].insert(0, test_interactions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/mind_small_train/small_test_negatives.tsv', 'w') as f:\n",
    "    for line in output:\n",
    "        line_str = '\\t'.join(str(ele) for ele in line) + \"\\n\"\n",
    "        f.write(line_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
