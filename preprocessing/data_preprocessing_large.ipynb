{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Cleaning of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from progressbar import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors = pd.read_csv(\"../../data/MINDlarge_train/behaviors.tsv\", sep='\\t', header=None)\n",
    "news = pd.read_csv(\"../../data/MINDlarge_train/news.tsv\", sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first give our datasets some proper column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors = behaviors.rename(columns={0:'impression_id', 1 : 'user_id', 2 : 'time', 3:'history', 4 : 'impressions'})\n",
    "news = news.rename(columns={0:'article_id', 1:'category', 2:'subcategory', 3:'title', 4:'abstract', 5:'url', 6:'title_entities', 7:'abstract_entities'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N88753</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N45436</td>\n",
       "      <td>news</td>\n",
       "      <td>newsscienceandtechnology</td>\n",
       "      <td>Walmart Slashes Prices on Last-Generation iPads</td>\n",
       "      <td>Apple's new iPad releases bring big deals on l...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AABmf2I.html</td>\n",
       "      <td>[{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...</td>\n",
       "      <td>[{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N23144</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id   category               subcategory  \\\n",
       "0     N88753  lifestyle           lifestyleroyals   \n",
       "1     N45436       news  newsscienceandtechnology   \n",
       "2     N23144     health                weightloss   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1    Walmart Slashes Prices on Last-Generation iPads   \n",
       "2                      50 Worst Habits For Belly Fat   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Shop the notebooks, jackets, and more that the...   \n",
       "1  Apple's new iPad releases bring big deals on l...   \n",
       "2  These seemingly harmless habits are holding yo...   \n",
       "\n",
       "                                             url  \\\n",
       "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
       "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "\n",
       "                                      title_entities  \\\n",
       "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
       "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "\n",
       "                                   abstract_entities  \n",
       "0                                                 []  \n",
       "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
       "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101527, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have more than 100,000 news articles in our *news* dataset with information concerning the **news category, subcategory, it's title, abstract and even some entitiy embeddings** (most of the urls don't work anymore so we don't have access to the full bodies). Let's check whether these are all unique articles or if we also have some duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique news articles:  98388\n",
      "Number of duplicates:              3139\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique news articles: \", news.title.nunique())\n",
    "print(\"Number of duplicates:             \", news.shape[0] - news.title.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, there are **news articles with multiple IDs**. We don't just want to drop them, because this would result in a loss of useful information concerning the click behaviors and reading histories in our *behaviors* dataset, which looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>time</th>\n",
       "      <th>history</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U87243</td>\n",
       "      <td>11/10/2019 11:30:54 AM</td>\n",
       "      <td>N8668 N39081 N65259 N79529 N73408 N43615 N2937...</td>\n",
       "      <td>N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U598644</td>\n",
       "      <td>11/12/2019 1:45:29 PM</td>\n",
       "      <td>N56056 N8726 N70353 N67998 N83823 N111108 N107...</td>\n",
       "      <td>N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U532401</td>\n",
       "      <td>11/13/2019 11:23:03 AM</td>\n",
       "      <td>N128643 N87446 N122948 N9375 N82348 N129412 N5...</td>\n",
       "      <td>N103852-0 N53474-0 N127836-0 N47925-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impression_id  user_id                    time  \\\n",
       "0              1   U87243  11/10/2019 11:30:54 AM   \n",
       "1              2  U598644   11/12/2019 1:45:29 PM   \n",
       "2              3  U532401  11/13/2019 11:23:03 AM   \n",
       "\n",
       "                                             history  \\\n",
       "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
       "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
       "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
       "\n",
       "                                         impressions  \n",
       "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
       "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
       "2              N103852-0 N53474-0 N127836-0 N47925-1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first prepare this dataset before we get back to handling the duplicate news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the *behaviors* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2232748, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have more than two million online sessions on msn with information concerning **user ID, date and daytime, the click history, and the recommended articles and user behavior** (ending on -1 = clicked) for the respective session. Let's see how many unique users there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 711222 individual users in our dataset.\n",
      "The average number of sessions is: 3.14\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(behaviors.user_id.unique())} individual users in our dataset.')\n",
    "print(f'The average number of sessions is: {behaviors.shape[0] / len(behaviors.user_id.unique()):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since we have to work with the click history, which was recorded before the sessions and is the same for all the sessions, as these two random samples show:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_id = behaviors[behaviors.user_id == 'U87243']\n",
    "len(behaviors_id.history), len(behaviors_id.history.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_id = behaviors[behaviors.user_id == 'U593596']\n",
    "len(behaviors_id.history), len(behaviors_id.history.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we want to include **only users with at least five articles read** in their history. So we need to drop ahistorical users as well as users with too few articles read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "impression_id        0\n",
       "user_id              0\n",
       "time                 0\n",
       "history          46065\n",
       "impressions          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2186683, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors['length_history'] = behaviors.history.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors['length_history'] = behaviors.length_history.map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors = behaviors[behaviors['length_history'] >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5      83726\n",
       "6      78792\n",
       "7      73876\n",
       "8      67973\n",
       "9      63602\n",
       "       ...  \n",
       "371        6\n",
       "317        5\n",
       "385        4\n",
       "404        2\n",
       "479        1\n",
       "Name: length_history, Length: 411, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors.length_history.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1940992"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With dropping those users, the numbers for our *behaviors* dataset now look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 575603 individual users in our dataset.\n",
      "The average number of sessions is: 3.37\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(behaviors.user_id.unique())} individual users in our dataset.')\n",
    "print(f'The average number of sessions is: {behaviors.shape[0] / len(behaviors.user_id.unique()):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Droppping duplicate article IDs in *news* and remapping them in *behaviors*\n",
    "Now that we have set up our *behaviors* dataset with respect to users, let's get back to the duplicate news articles. With different IDs for the de facto same articles we would not be able to track similarities among users sufficiently. In the following, we will **replace every redundant article-ID with the first ID for the respective article**. In order to this, we first create a subset with all the duplicates in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplis_title = news[news.duplicated(subset=\"title\", keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22698</th>\n",
       "      <td>N11490</td>\n",
       "      <td>travel</td>\n",
       "      <td>travelnews</td>\n",
       "      <td>$2 million Florida Lottery ticket sold in Jack...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJLM24.html</td>\n",
       "      <td>[{\"Label\": \"Florida Lottery\", \"Type\": \"O\", \"Wi...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43099</th>\n",
       "      <td>N33885</td>\n",
       "      <td>travel</td>\n",
       "      <td>travelnews</td>\n",
       "      <td>$2 million Florida Lottery ticket sold in Jack...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJTMIW.html</td>\n",
       "      <td>[{\"Label\": \"Florida Lottery\", \"Type\": \"O\", \"Wi...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53996</th>\n",
       "      <td>N88198</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestylebuzz</td>\n",
       "      <td>$23 Million SuperLotto Plus Ticket Sold In San...</td>\n",
       "      <td>Are you holding the winning ticket worth $23 m...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJCjxB.html</td>\n",
       "      <td>[{\"Label\": \"San Fernando Valley\", \"Type\": \"L\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id   category    subcategory  \\\n",
       "22698     N11490     travel     travelnews   \n",
       "43099     N33885     travel     travelnews   \n",
       "53996     N88198  lifestyle  lifestylebuzz   \n",
       "\n",
       "                                                   title  \\\n",
       "22698  $2 million Florida Lottery ticket sold in Jack...   \n",
       "43099  $2 million Florida Lottery ticket sold in Jack...   \n",
       "53996  $23 Million SuperLotto Plus Ticket Sold In San...   \n",
       "\n",
       "                                                abstract  \\\n",
       "22698                                                NaN   \n",
       "43099                                                NaN   \n",
       "53996  Are you holding the winning ticket worth $23 m...   \n",
       "\n",
       "                                                 url  \\\n",
       "22698  https://assets.msn.com/labs/mind/AAJLM24.html   \n",
       "43099  https://assets.msn.com/labs/mind/AAJTMIW.html   \n",
       "53996  https://assets.msn.com/labs/mind/AAJCjxB.html   \n",
       "\n",
       "                                          title_entities abstract_entities  \n",
       "22698  [{\"Label\": \"Florida Lottery\", \"Type\": \"O\", \"Wi...                []  \n",
       "43099  [{\"Label\": \"Florida Lottery\", \"Type\": \"O\", \"Wi...                []  \n",
       "53996  [{\"Label\": \"San Fernando Valley\", \"Type\": \"L\",...                []  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplis_title.sort_values(by=\"title\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_set = duplis_title['title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = []\n",
    "for title in title_set:\n",
    "    x = duplis_title[duplis_title['title']==title]['article_id'].to_list()\n",
    "    article_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['N93333', 'N18950'],\n",
       " ['N108072', 'N19710'],\n",
       " ['N112099', 'N50263'],\n",
       " ['N108765', 'N123453'],\n",
       " ['N1396', 'N45124']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a **list which contains article IDs for every article which has multiple IDs** in our original dataset. With this list, we can generate a dictionary called articleID_dict, which maps all the redundant IDs (keys) to a single ID (value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "articleID_dict = {}\n",
    "articles_to_change = []\n",
    "for article in article_list:\n",
    "    value = article[0]\n",
    "    keys = article [1:]\n",
    "    for k in keys:\n",
    "        articleID_dict[k] = value\n",
    "        articles_to_change.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a copy of the original behaviors dataframe and make it to a numpy array, so that we can **loop through all the redundant IDs in the *behaviors* dataset and homogenize them** according to our dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav = behaviors.copy()\n",
    "behav = behav.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = ProgressBar()\n",
    "userIDs_hist_changes = []\n",
    "userIDs_impr_changes = []\n",
    "users_to_change = []\n",
    "articles_to_change_set = set(articles_to_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "for idx in pbar(range(behav.shape[0])):\n",
    "    user_row = behav[idx]\n",
    "    hist_flag = False\n",
    "    hist = user_row[3]\n",
    "    hist_list = hist.split()\n",
    "    hist_set = set(hist_list)\n",
    "    hist_inter = hist_set & articles_to_change_set\n",
    "    for art in hist_inter:\n",
    "        hist_flag = True\n",
    "        users_to_change.append(idx)\n",
    "        userIDs_hist_changes.append(user_row[1])\n",
    "        hist = hist.replace(art, articleID_dict[art])\n",
    "    if hist_flag:\n",
    "        behav[idx][3] = hist\n",
    "    impression_flag = False\n",
    "    impressions = user_row[4]\n",
    "    impression_list = [l[:-2] for l in impressions.split()]\n",
    "    impression_set = set(impression_list)\n",
    "    for art in (impression_set & articles_to_change_set):\n",
    "        impression_flag = True\n",
    "        userIDs_impr_changes.append(user_row[1])\n",
    "        impressions = impressions.replace(art, articleID_dict[art])\n",
    "    if impression_flag:        \n",
    "        behav[idx][4] = impressions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's **check wether our method worked**. For this task, we **construct a list containing all articles in our array and compare it to the articles_to_change list**, by making them to sets and calculating their intersection, which should be empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = []\n",
    "for row in behav:\n",
    "    hist_list=row[3].split(' ')\n",
    "    for article in hist_list:\n",
    "        all_articles.append(article)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(all_articles) & articles_to_change_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, our method worked! Now let's make a **new dataframe out of our processed behavioral data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_new = pd.DataFrame(behav, columns= behaviors.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a new one for the information on **news articles without duplicates**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_new = news.drop_duplicates(subset=\"title\", keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving processed datasets\n",
    "Now we want to save the processed data and write it to csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_new.to_csv(\"../../data/MINDlarge_train/behaviors_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_new.to_csv(\"../../data/MINDlarge_train/news_processed.csv\", \n",
    "                    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for collaborative filtering approaches\n",
    "For the deployment of recommender systems which use collaborative filtering (CF) techniques, user-article interactions play a pivotal role. Because CF is of great importance to understand modern day recommender systems in general, we too want to construct and discuss different versions of this approach. In order to do this, it is useful to further process our data with repsect to user-article interactions. \n",
    "\n",
    "Because we **only work with the click history** when deploying CF methods, we only need one session per user:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_cf = behaviors_new.drop_duplicates(subset='user_id').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_cf.shape[0] == behaviors_new.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>time</th>\n",
       "      <th>history</th>\n",
       "      <th>impressions</th>\n",
       "      <th>length_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U87243</td>\n",
       "      <td>11/10/2019 11:30:54 AM</td>\n",
       "      <td>N8668 N39081 N65259 N79529 N73408 N43615 N2937...</td>\n",
       "      <td>N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U598644</td>\n",
       "      <td>11/12/2019 1:45:29 PM</td>\n",
       "      <td>N56056 N8726 N70353 N67998 N83823 N111108 N107...</td>\n",
       "      <td>N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U532401</td>\n",
       "      <td>11/13/2019 11:23:03 AM</td>\n",
       "      <td>N128643 N87446 N122948 N9375 N82348 N129412 N5...</td>\n",
       "      <td>N103852-0 N53474-0 N127836-0 N47925-1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>U593596</td>\n",
       "      <td>11/12/2019 12:24:09 PM</td>\n",
       "      <td>N31043 N39592 N4104 N8223 N114581 N92747 N1207...</td>\n",
       "      <td>N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>U239687</td>\n",
       "      <td>11/14/2019 8:03:01 PM</td>\n",
       "      <td>N65250 N122359 N71723 N53796 N41663 N41484 N11...</td>\n",
       "      <td>N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  impression_id  user_id                    time  \\\n",
       "0             1   U87243  11/10/2019 11:30:54 AM   \n",
       "1             2  U598644   11/12/2019 1:45:29 PM   \n",
       "2             3  U532401  11/13/2019 11:23:03 AM   \n",
       "3             4  U593596  11/12/2019 12:24:09 PM   \n",
       "4             5  U239687   11/14/2019 8:03:01 PM   \n",
       "\n",
       "                                             history  \\\n",
       "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
       "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
       "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
       "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
       "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
       "\n",
       "                                         impressions length_history  \n",
       "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...             16  \n",
       "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...             24  \n",
       "2              N103852-0 N53474-0 N127836-0 N47925-1             16  \n",
       "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...             13  \n",
       "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...            339  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_cf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to construct a numpy array out of this smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_cf = behaviors_cf.to_numpy(copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so that we can get **two arrays with user-article-interactions**. One for training and another one with the last article in history for testing purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_train, uai_test = [], []\n",
    "\n",
    "for row in behav_cf:\n",
    "    user = row[1]\n",
    "    hist = row[3].split(' ')\n",
    "    for art in hist[:-1]:\n",
    "        uai_train.append([user, art])\n",
    "    last_art = hist[-1]\n",
    "    uai_test.append([user, last_art])\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if that worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(behaviors.loc[0].history.split(' ')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['U87243', 'N8668'],\n",
       "  ['U87243', 'N39081'],\n",
       "  ['U87243', 'N65259'],\n",
       "  ['U87243', 'N79529'],\n",
       "  ['U87243', 'N73408'],\n",
       "  ['U87243', 'N43615'],\n",
       "  ['U87243', 'N29379'],\n",
       "  ['U87243', 'N32031'],\n",
       "  ['U87243', 'N110232'],\n",
       "  ['U87243', 'N101921'],\n",
       "  ['U87243', 'N12614'],\n",
       "  ['U87243', 'N129591'],\n",
       "  ['U87243', 'N105760'],\n",
       "  ['U87243', 'N60457'],\n",
       "  ['U87243', 'N1229']],\n",
       " ['U87243', 'N64932'])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uai_train[0:15], uai_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good! Now we want to get some **extra user- and article integer IDs**, that we can later use for a **dictionary-of-keys-matrix**, which in turn will be **employed in a neural network**. For our train data, we can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_train_df = pd.DataFrame(uai_train, columns=['user_id', 'article_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_train_df['user_int_id'] = uai_train_df.user_id.astype('category').cat.codes\n",
    "uai_train_df['article_int_id'] = uai_train_df.article_id.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>user_int_id</th>\n",
       "      <th>article_int_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U87243</td>\n",
       "      <td>N8668</td>\n",
       "      <td>564162</td>\n",
       "      <td>67522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U87243</td>\n",
       "      <td>N39081</td>\n",
       "      <td>564162</td>\n",
       "      <td>36610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U87243</td>\n",
       "      <td>N65259</td>\n",
       "      <td>564162</td>\n",
       "      <td>53587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id article_id  user_int_id  article_int_id\n",
       "0  U87243      N8668       564162           67522\n",
       "1  U87243     N39081       564162           36610\n",
       "2  U87243     N65259       564162           53587"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uai_train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for our test data, we need to make sure that it contains only the articles which are also in the train data. At first, we need to find those articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_articles = [elem[1] for elem in uai_train]\n",
    "test_articles = [elem[1] for elem in uai_test]\n",
    "articles_to_drop = set(test_articles)-set(train_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this list, we're now able to reduce our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_test_red = [ele for ele in uai_test if ele[1] not in articles_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and can thus create a test dataframe without unknown articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_test_df = pd.DataFrame(uai_test_red, columns=['user_id', 'article_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then construct two dictionaries, which map the original user and article IDs to the integer IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_code_dict = pd.Series(uai_train_df.user_int_id.values,\n",
    "                           index=uai_train_df.user_id).to_dict()\n",
    "\n",
    "article_code_dict = pd.Series(uai_train_df.article_int_id.values,\n",
    "                              index=uai_train_df.article_id).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_test_df['user_int_id'] = [user_code_dict[user] for user in uai_test_df.user_id]\n",
    "uai_test_df['article_int_id'] = [article_code_dict[art] for art in uai_test_df.article_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>user_int_id</th>\n",
       "      <th>article_int_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U87243</td>\n",
       "      <td>N64932</td>\n",
       "      <td>564162</td>\n",
       "      <td>53345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U598644</td>\n",
       "      <td>N31055</td>\n",
       "      <td>448422</td>\n",
       "      <td>31394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U532401</td>\n",
       "      <td>N31323</td>\n",
       "      <td>388857</td>\n",
       "      <td>31569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id article_id  user_int_id  article_int_id\n",
       "0   U87243     N64932       564162           53345\n",
       "1  U598644     N31055       448422           31394\n",
       "2  U532401     N31323       388857           31569"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uai_test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_train_df.to_csv(\"../../data/MINDlarge_train/large_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_test_df.to_csv(\"../../data/MINDlarge_train/large_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actually **create the dok-matrix**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = \"../../data/MINDlarge_train/large_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(575603, 76236)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users, num_articles = uai_train_df.user_id.nunique(), uai_train_df.article_id.nunique()\n",
    "num_users, num_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id,article_id,user_int_id,article_int_id\n",
      "\n",
      "U87243,N8668,564162,67522\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_matrix = sp.dok_matrix((num_users, num_articles), dtype=np.float32)\n",
    "\n",
    "with open(train_filename, \"r\") as f:\n",
    "    header = f.readline()\n",
    "    line = f.readline()\n",
    "    print(header)\n",
    "    print(line)\n",
    "    while line != None and line != \"\":\n",
    "        line_list = line.split(\",\")\n",
    "        user, article = int(line_list[2]), int(line_list[3])\n",
    "        train_matrix[user, article] = 1.0\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, when we want to evaluate our recommender systems, we need to **compare the ranking for the known -- but not learned -- interactions with known non-interactions**, so that we can tell how useful our recommendation is: the higher the ranking of the test interaction, the better our model! In order to do this, we want to **extract 99 non read articles for every user in our test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_interactions = list(zip(uai_test_df.user_int_id, uai_test_df.article_int_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_negatives = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_interactions = []\n",
    "for u, i in test_interactions:\n",
    "    negatives = []\n",
    "    for t in range(num_negatives):\n",
    "        j = np.random.randint(num_articles)\n",
    "        while (u, j) in train_matrix.keys():\n",
    "            j = np.random.randint(num_articles)\n",
    "        negatives.append(j)\n",
    "    negative_interactions.append(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(574780, 574780, 99)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_interactions), len(negative_interactions), len(negative_interactions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally**, we want to **write our one positive interaction along with the randomly generated non interactions into a csv file** and we're done with preprocessing the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = negative_interactions[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_interactions)):\n",
    "    output[i].insert(0, test_interactions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/MINDlarge_train/large_test_negatives.tsv', 'w') as f:\n",
    "    for line in output:\n",
    "        line_str = '\\t'.join(str(ele) for ele in line) + \"\\n\"\n",
    "        f.write(line_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
