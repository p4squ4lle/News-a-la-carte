{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Cleaning of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from progressbar import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose whether to load the small or large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_matrix = False\n",
    "\n",
    "# Choose from \"small\" or \"large\"\n",
    "dataset_size = \"large\"\n",
    "# Choose from \"train\" or \"dev\" (test)\n",
    "dataset_type = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = f\"../../data/mind_{dataset_size}_{dataset_type}/\"\n",
    "behaviors_path = dataset_path + \"behaviors.tsv\"\n",
    "news_path = dataset_path + \"news.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors = pd.read_csv(behaviors_path, sep='\\t', header=None)\n",
    "news = pd.read_csv(news_path, sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first give our datasets some proper column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors = behaviors.rename(columns={0: 'impression_id', \n",
    "                                      1: 'user_id', \n",
    "                                      2: 'time', \n",
    "                                      3: 'history', \n",
    "                                      4: 'impressions'})\n",
    "\n",
    "news = news.rename(columns={0: 'article_id', \n",
    "                            1: 'category', \n",
    "                            2: 'subcategory', \n",
    "                            3: 'title', \n",
    "                            4: 'abstract', \n",
    "                            5: 'url', \n",
    "                            6: 'title_entities', \n",
    "                            7: 'abstract_entities'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N88753</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N45436</td>\n",
       "      <td>news</td>\n",
       "      <td>newsscienceandtechnology</td>\n",
       "      <td>Walmart Slashes Prices on Last-Generation iPads</td>\n",
       "      <td>Apple's new iPad releases bring big deals on l...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AABmf2I.html</td>\n",
       "      <td>[{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...</td>\n",
       "      <td>[{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N23144</td>\n",
       "      <td>health</td>\n",
       "      <td>weightloss</td>\n",
       "      <td>50 Worst Habits For Belly Fat</td>\n",
       "      <td>These seemingly harmless habits are holding yo...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAB19MK.html</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "      <td>[{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id   category               subcategory  \\\n",
       "0     N88753  lifestyle           lifestyleroyals   \n",
       "1     N45436       news  newsscienceandtechnology   \n",
       "2     N23144     health                weightloss   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1    Walmart Slashes Prices on Last-Generation iPads   \n",
       "2                      50 Worst Habits For Belly Fat   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Shop the notebooks, jackets, and more that the...   \n",
       "1  Apple's new iPad releases bring big deals on l...   \n",
       "2  These seemingly harmless habits are holding yo...   \n",
       "\n",
       "                                             url  \\\n",
       "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1  https://assets.msn.com/labs/mind/AABmf2I.html   \n",
       "2  https://assets.msn.com/labs/mind/AAB19MK.html   \n",
       "\n",
       "                                      title_entities  \\\n",
       "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...   \n",
       "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...   \n",
       "\n",
       "                                   abstract_entities  \n",
       "0                                                 []  \n",
       "1  [{\"Label\": \"IPad\", \"Type\": \"J\", \"WikidataId\": ...  \n",
       "2  [{\"Label\": \"Adipose tissue\", \"Type\": \"C\", \"Wik...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of news dataset: (101527, 8)\n",
      "There are more than 101,000 news articles in our news dataset.\n"
     ]
    }
   ],
   "source": [
    "news_shape = news.shape\n",
    "print(f\"\\nShape of news dataset: {news_shape}\")\n",
    "print(f\"There are more than {news_shape[0]//1000},000 news articles in our news dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every article we have information concerning the **news category, subcategory, it's title, abstract and even some entitiy embeddings** (most of the urls don't work anymore so we don't have access to the full bodies). Let's check whether these are all unique articles or if we also have some duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique news articles:  98388\n",
      "Number of duplicates:              3139\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique news articles: \", news.title.nunique())\n",
    "print(\"Number of duplicates:             \", news.shape[0] - news.title.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, there are **news articles with multiple IDs**. We don't just want to drop them yet, as this would result in a loss of useful information concerning the click behaviors and reading histories in our ***behaviors* dataset**, which looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>time</th>\n",
       "      <th>history</th>\n",
       "      <th>impressions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U87243</td>\n",
       "      <td>11/10/2019 11:30:54 AM</td>\n",
       "      <td>N8668 N39081 N65259 N79529 N73408 N43615 N2937...</td>\n",
       "      <td>N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U598644</td>\n",
       "      <td>11/12/2019 1:45:29 PM</td>\n",
       "      <td>N56056 N8726 N70353 N67998 N83823 N111108 N107...</td>\n",
       "      <td>N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U532401</td>\n",
       "      <td>11/13/2019 11:23:03 AM</td>\n",
       "      <td>N128643 N87446 N122948 N9375 N82348 N129412 N5...</td>\n",
       "      <td>N103852-0 N53474-0 N127836-0 N47925-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   impression_id  user_id                    time  \\\n",
       "0              1   U87243  11/10/2019 11:30:54 AM   \n",
       "1              2  U598644   11/12/2019 1:45:29 PM   \n",
       "2              3  U532401  11/13/2019 11:23:03 AM   \n",
       "\n",
       "                                             history  \\\n",
       "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
       "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
       "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
       "\n",
       "                                         impressions  \n",
       "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...  \n",
       "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...  \n",
       "2              N103852-0 N53474-0 N127836-0 N47925-1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first prepare this dataset before we get back to handling the duplicate news articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the *behaviors* dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of behaviors dataset: (2232748, 5)\n",
      "In the behaviors dataset there are more than 2232,000 online sessions from MSN news.\n"
     ]
    }
   ],
   "source": [
    "behaviors_shape = behaviors.shape\n",
    "print(f\"\\nShape of behaviors dataset: {behaviors_shape}\")\n",
    "print(f\"In the behaviors dataset there are more than {behaviors_shape[0]//1000},000\",\n",
    "      \"online sessions from MSN news.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have information concerning **user ID, date and daytime, the click history, and the recommended articles and user behavior** (ending on -1 = clicked) for the respective session. Let's see how many unique users there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 711222 individual users in our dataset.\n",
      "The average number of sessions is: 3.1\n"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(behaviors.user_id.unique())} individual users in our dataset.')\n",
    "print(f'The average number of sessions is: {behaviors_shape[0] / len(behaviors.user_id.unique()):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The click history of users with multiple sessions is the same for all sessions, as the following two random samples show:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U536528    106\n",
      "U569581     67\n",
      "U41536      66\n",
      "U301043     62\n",
      "U483745     60\n",
      "Name: user_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "top5_multiple_session_users = behaviors.user_id.value_counts()[:5]\n",
    "print(top5_multiple_session_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U536528'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_multiple_session_users.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_id = behaviors[behaviors.user_id == top5_multiple_session_users.index[0]]\n",
    "len(behaviors_id.history), len(behaviors_id.history.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_id = behaviors[behaviors.user_id == top5_multiple_session_users.index[1]]\n",
    "len(behaviors_id.history), len(behaviors_id.history.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we work with the click history quite a bit,we want to include **only users with at least five articles read** in their history. So we need to drop ahistorical users as well as users with too few articles read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "impression_id        0\n",
       "user_id              0\n",
       "time                 0\n",
       "history          46065\n",
       "impressions          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors['length_history'] = behaviors.history.str.split().map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors = behaviors[behaviors['length_history'] >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    83726\n",
       "6    78792\n",
       "7    73876\n",
       "8    67973\n",
       "9    63602\n",
       "Name: length_history, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors.length_history.value_counts()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Droppping duplicate article IDs in *news* and remapping them in *behaviors*\n",
    "Now that we have set up our *behaviors* dataset with respect to users, let's get back to the duplicate news articles. With different IDs for the de facto same articles we would not be able to track similarities among users sufficiently. In the following, we will **replace every redundant article-ID with the first ID for the respective article**. In order to this, we first create a subset with all the duplicates in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplis_title = news[news.duplicated(subset=\"title\", keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5367, 8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplis_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22698</th>\n",
       "      <td>N11490</td>\n",
       "      <td>travel</td>\n",
       "      <td>travelnews</td>\n",
       "      <td>$2 million Florida Lottery ticket sold in Jack...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJLM24.html</td>\n",
       "      <td>[{\"Label\": \"Florida Lottery\", \"Type\": \"O\", \"Wi...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43099</th>\n",
       "      <td>N33885</td>\n",
       "      <td>travel</td>\n",
       "      <td>travelnews</td>\n",
       "      <td>$2 million Florida Lottery ticket sold in Jack...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJTMIW.html</td>\n",
       "      <td>[{\"Label\": \"Florida Lottery\", \"Type\": \"O\", \"Wi...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53996</th>\n",
       "      <td>N88198</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestylebuzz</td>\n",
       "      <td>$23 Million SuperLotto Plus Ticket Sold In San...</td>\n",
       "      <td>Are you holding the winning ticket worth $23 m...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJCjxB.html</td>\n",
       "      <td>[{\"Label\": \"San Fernando Valley\", \"Type\": \"L\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id   category    subcategory  \\\n",
       "22698     N11490     travel     travelnews   \n",
       "43099     N33885     travel     travelnews   \n",
       "53996     N88198  lifestyle  lifestylebuzz   \n",
       "\n",
       "                                                   title  \\\n",
       "22698  $2 million Florida Lottery ticket sold in Jack...   \n",
       "43099  $2 million Florida Lottery ticket sold in Jack...   \n",
       "53996  $23 Million SuperLotto Plus Ticket Sold In San...   \n",
       "\n",
       "                                                abstract  \\\n",
       "22698                                                NaN   \n",
       "43099                                                NaN   \n",
       "53996  Are you holding the winning ticket worth $23 m...   \n",
       "\n",
       "                                                 url  \\\n",
       "22698  https://assets.msn.com/labs/mind/AAJLM24.html   \n",
       "43099  https://assets.msn.com/labs/mind/AAJTMIW.html   \n",
       "53996  https://assets.msn.com/labs/mind/AAJCjxB.html   \n",
       "\n",
       "                                          title_entities abstract_entities  \n",
       "22698  [{\"Label\": \"Florida Lottery\", \"Type\": \"O\", \"Wi...                []  \n",
       "43099  [{\"Label\": \"Florida Lottery\", \"Type\": \"O\", \"Wi...                []  \n",
       "53996  [{\"Label\": \"San Fernando Valley\", \"Type\": \"L\",...                []  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplis_title.sort_values(by=\"title\").head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_set = duplis_title['title'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = []\n",
    "for title in title_set:\n",
    "    x = duplis_title[duplis_title['title']==title]['article_id'].to_list()\n",
    "    article_list.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['N93333', 'N18950'],\n",
       " ['N108072', 'N19710'],\n",
       " ['N112099', 'N50263'],\n",
       " ['N108765', 'N123453'],\n",
       " ['N1396', 'N45124']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a **list which contains article IDs for every article which has multiple IDs** in our original dataset. With this list, we can generate a dictionary called articleID_dict, which maps all the redundant IDs (keys) to a single ID (value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "articleID_dict = {}\n",
    "articles_to_change = []\n",
    "for article in article_list:\n",
    "    value = article[0]\n",
    "    keys = article [1:]\n",
    "    for k in keys:\n",
    "        articleID_dict[k] = value\n",
    "        articles_to_change.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a copy of the original behaviors dataframe and make it to a numpy array, so that we can **loop through all the redundant IDs in the *behaviors* dataset and homogenize them** according to our dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav = behaviors.copy()\n",
    "behav = behav.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = ProgressBar()\n",
    "userIDs_hist_changes = []\n",
    "userIDs_impr_changes = []\n",
    "users_to_change = []\n",
    "articles_to_change_set = set(articles_to_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% |########################################################################|\n"
     ]
    }
   ],
   "source": [
    "for idx in pbar(range(behav.shape[0])):\n",
    "    user_row = behav[idx]\n",
    "    hist_flag = False\n",
    "    hist = user_row[3]\n",
    "    hist_list = hist.split()\n",
    "    hist_set = set(hist_list)\n",
    "    hist_inter = hist_set & articles_to_change_set\n",
    "    for art in hist_inter:\n",
    "        hist_flag = True\n",
    "        users_to_change.append(idx)\n",
    "        userIDs_hist_changes.append(user_row[1])\n",
    "        hist = hist.replace(art, articleID_dict[art])\n",
    "    if hist_flag:\n",
    "        behav[idx][3] = hist\n",
    "    impression_flag = False\n",
    "    impressions = user_row[4]\n",
    "    impression_list = [l[:-2] for l in impressions.split()]\n",
    "    impression_set = set(impression_list)\n",
    "    for art in (impression_set & articles_to_change_set):\n",
    "        impression_flag = True\n",
    "        userIDs_impr_changes.append(user_row[1])\n",
    "        impressions = impressions.replace(art, articleID_dict[art])\n",
    "    if impression_flag:        \n",
    "        behav[idx][4] = impressions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's **check wether our method worked**. For this task, we **construct a list containing all articles in our array and compare it to the articles_to_change list**, by making them to sets and calculating their intersection, which should be empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = []\n",
    "for row in behav:\n",
    "    hist_list = row[3].split(' ')\n",
    "    for article in hist_list:\n",
    "        all_articles.append(article)\n",
    "    impr_list = row[4].split(' ')\n",
    "    for article in impr_list:\n",
    "        all_articles.append(article[:-2])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(all_articles) & articles_to_change_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, our method worked! \n",
    "\n",
    "## Dropping users who read articles on which there is no information\n",
    "\n",
    "We realized, that there are some articles in the history and impression logs without a corresponding entry in the news dataset. So, before saving the processed datasets, we also remove all the users in the behaviors dataset, who read those articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_articles_behav = set(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_articles_news = set(news.article_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_to_drop = unique_articles_behav - unique_articles_news\n",
    "len(articles_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [article_id, category, subcategory, title, abstract, url, title_entities, abstract_entities]\n",
       "Index: []"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[news.article_id.isin(articles_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['impression_id', 'user_id', 'time', 'history', 'impressions',\n",
       "       'length_history'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_to_drop = []\n",
    "for i, impr in enumerate(behav):\n",
    "    hist_set = set(impr[3].split())\n",
    "    if len(hist_set & articles_to_drop) > 0:\n",
    "        users_to_drop.append(i)\n",
    "    \n",
    "    impr_set = set([art[:-2] for art in impr[4].split()])\n",
    "    if len(impr_set & articles_to_drop) > 0:\n",
    "        users_to_drop.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_to_drop = list(set(users_to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_new = np.delete(behav, users_to_drop, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behav.shape[0] - behav_new.shape[0] == len(users_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a **new dataframe out of our processed behavioral data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_new = pd.DataFrame(behav_new, columns=behaviors.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(behaviors_new.user_id.unique()) & set(users_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the processing from above, the numbers for our *behaviors* dataset now look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now just over 1940,000 sessions and 575553 individual users in our dataset.\n",
      "The average number of sessions is: 3.4\n"
     ]
    }
   ],
   "source": [
    "print(f'There are now just over {behaviors_new.shape[0]//1000},000 sessions and {len(behaviors_new.user_id.unique())}',\n",
    "      'individual users in our dataset.')\n",
    "print(f'The average number of sessions is: {behaviors_new.shape[0] / len(behaviors_new.user_id.unique()):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also make a new dataframe for the information on **news articles without duplicates**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_new = news.drop_duplicates(subset=\"title\", keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98388, 8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving processed datasets\n",
    "Now we want to save the processed data and write it to csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_output_path = dataset_path + \"behaviors_processed.csv\"\n",
    "behaviors_new.to_csv(behaviors_output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_output_path = dataset_path + \"news_processed.csv\"\n",
    "news_new.to_csv(news_output_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing for collaborative filtering approaches\n",
    "For the deployment of recommender systems which use Collaborative Filtering (CF) techniques, user-article interactions play a pivotal role. Because CF is of great importance to understand modern day recommender systems in general, we too want to construct and discuss different versions of this approach. In order to do this, it is useful to further process our data with repsect to user-article interactions. \n",
    "\n",
    "Because we **only work with the click history** when deploying CF methods, we only need one session per user:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "behaviors_cf = behaviors_new.drop_duplicates(subset='user_id').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert behaviors_cf.shape[0] == behaviors_new.user_id.nunique(), \\\n",
    "       \"User duplicates have not been dropped\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>impression_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>time</th>\n",
       "      <th>history</th>\n",
       "      <th>impressions</th>\n",
       "      <th>length_history</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>U87243</td>\n",
       "      <td>11/10/2019 11:30:54 AM</td>\n",
       "      <td>N8668 N39081 N65259 N79529 N73408 N43615 N2937...</td>\n",
       "      <td>N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>U598644</td>\n",
       "      <td>11/12/2019 1:45:29 PM</td>\n",
       "      <td>N56056 N8726 N70353 N67998 N83823 N111108 N107...</td>\n",
       "      <td>N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>U532401</td>\n",
       "      <td>11/13/2019 11:23:03 AM</td>\n",
       "      <td>N128643 N87446 N122948 N9375 N82348 N129412 N5...</td>\n",
       "      <td>N103852-0 N53474-0 N127836-0 N47925-1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>U593596</td>\n",
       "      <td>11/12/2019 12:24:09 PM</td>\n",
       "      <td>N31043 N39592 N4104 N8223 N114581 N92747 N1207...</td>\n",
       "      <td>N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>U239687</td>\n",
       "      <td>11/14/2019 8:03:01 PM</td>\n",
       "      <td>N65250 N122359 N71723 N53796 N41663 N41484 N11...</td>\n",
       "      <td>N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...</td>\n",
       "      <td>339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  impression_id  user_id                    time  \\\n",
       "0             1   U87243  11/10/2019 11:30:54 AM   \n",
       "1             2  U598644   11/12/2019 1:45:29 PM   \n",
       "2             3  U532401  11/13/2019 11:23:03 AM   \n",
       "3             4  U593596  11/12/2019 12:24:09 PM   \n",
       "4             5  U239687   11/14/2019 8:03:01 PM   \n",
       "\n",
       "                                             history  \\\n",
       "0  N8668 N39081 N65259 N79529 N73408 N43615 N2937...   \n",
       "1  N56056 N8726 N70353 N67998 N83823 N111108 N107...   \n",
       "2  N128643 N87446 N122948 N9375 N82348 N129412 N5...   \n",
       "3  N31043 N39592 N4104 N8223 N114581 N92747 N1207...   \n",
       "4  N65250 N122359 N71723 N53796 N41663 N41484 N11...   \n",
       "\n",
       "                                         impressions length_history  \n",
       "0  N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N5...             16  \n",
       "1  N47996-0 N82719-0 N117066-0 N8491-0 N123784-0 ...             24  \n",
       "2              N103852-0 N53474-0 N127836-0 N47925-1             16  \n",
       "3  N38902-0 N76434-0 N71593-0 N100073-0 N108736-0...             13  \n",
       "4  N76209-0 N48841-0 N67937-0 N62235-0 N6307-0 N3...            339  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behaviors_cf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to construct a numpy array out of this smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_cf = behaviors_cf.to_numpy(copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so that we can get **two lists with user-article-interactions**. One for training and another one with the last article in history for testing purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impression_id,user_id,time,history,impressions,length_history\n",
      "\n",
      "1,U87243,11/10/2019 11:30:54 AM,N8668 N39081 N65259 N79529 N73408 N43615 N29379 N32031 N110232 N101921 N12614 N129591 N105760 N60457 N1229 N64932,N78206-0 N26368-0 N7578-0 N58592-0 N19858-0 N58258-0 N18478-0 N2591-0 N97778-0 N32954-0 N94157-1 N39404-0 N108809-0 N78699-1 N71090-1 N40282-0 N31174-1 N37924-0 N27822-0,16\n",
      "\n",
      "1940492\r"
     ]
    }
   ],
   "source": [
    "uai_train, uai_test = [], []\n",
    "i = 0\n",
    "with open(behaviors_output_path, \"r\") as f:\n",
    "    header = f.readline()\n",
    "    line = f.readline()\n",
    "    print(header)\n",
    "    print(line)\n",
    "    while line != None and line != \"\":\n",
    "        i += 1\n",
    "        print(i, end=\"\\r\")\n",
    "        row = line.split(\",\")\n",
    "        user = row[1]\n",
    "        hist = row[3].split(' ')\n",
    "        for art in hist[:-1]:\n",
    "            uai_train.append([user, art])\n",
    "            \n",
    "        last_art = hist[-1]\n",
    "        uai_test.append([user, last_art])\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if that worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(behaviors.loc[0].history.split(' ')[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['U87243', 'N8668'],\n",
       "  ['U87243', 'N39081'],\n",
       "  ['U87243', 'N65259'],\n",
       "  ['U87243', 'N79529'],\n",
       "  ['U87243', 'N73408'],\n",
       "  ['U87243', 'N43615'],\n",
       "  ['U87243', 'N29379'],\n",
       "  ['U87243', 'N32031'],\n",
       "  ['U87243', 'N110232'],\n",
       "  ['U87243', 'N101921'],\n",
       "  ['U87243', 'N12614'],\n",
       "  ['U87243', 'N129591'],\n",
       "  ['U87243', 'N105760'],\n",
       "  ['U87243', 'N60457'],\n",
       "  ['U87243', 'N1229']],\n",
       " ['U87243', 'N64932'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uai_train[0:15], uai_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good! Now we want to get some **extra user- and article integer IDs**, that we can later use for a **dictionary-of-keys-matrix**, which in turn will be **employed in a neural network**. For our train data, we can do it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_train_df = pd.DataFrame(uai_train, columns=['user_id', 'article_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_train_df['user_int_id'] = uai_train_df.user_id.astype('category').cat.codes\n",
    "uai_train_df['article_int_id'] = uai_train_df.article_id.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>user_int_id</th>\n",
       "      <th>article_int_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U87243</td>\n",
       "      <td>N8668</td>\n",
       "      <td>564112</td>\n",
       "      <td>67478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U87243</td>\n",
       "      <td>N39081</td>\n",
       "      <td>564112</td>\n",
       "      <td>36583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U87243</td>\n",
       "      <td>N65259</td>\n",
       "      <td>564112</td>\n",
       "      <td>53550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id article_id  user_int_id  article_int_id\n",
       "0  U87243      N8668       564112           67478\n",
       "1  U87243     N39081       564112           36583\n",
       "2  U87243     N65259       564112           53550"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uai_train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for our test data, we need to make sure that it contains only the articles which are also in the train data. At first, we need to find those articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_articles = [elem[1] for elem in uai_train]\n",
    "test_articles = [elem[1] for elem in uai_test]\n",
    "articles_to_drop = set(test_articles)-set(train_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this list, we're now able to reduce our test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_test_red = [ele for ele in uai_test if ele[1] not in articles_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and can thus create a test dataframe without unknown articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_test_df = pd.DataFrame(uai_test_red, columns=['user_id', 'article_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then construct two dictionaries, which map the original user and article IDs to the integer IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_code_dict = pd.Series(uai_train_df.user_int_id.values,\n",
    "                           index=uai_train_df.user_id).to_dict()\n",
    "\n",
    "article_code_dict = pd.Series(uai_train_df.article_int_id.values,\n",
    "                              index=uai_train_df.article_id).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_test_df['user_int_id'] = [user_code_dict[user] for user in uai_test_df.user_id]\n",
    "uai_test_df['article_int_id'] = [article_code_dict[art] for art in uai_test_df.article_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>user_int_id</th>\n",
       "      <th>article_int_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U87243</td>\n",
       "      <td>N64932</td>\n",
       "      <td>564112</td>\n",
       "      <td>53308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U598644</td>\n",
       "      <td>N31055</td>\n",
       "      <td>448386</td>\n",
       "      <td>31370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U532401</td>\n",
       "      <td>N31323</td>\n",
       "      <td>388825</td>\n",
       "      <td>31545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id article_id  user_int_id  article_int_id\n",
       "0   U87243     N64932       564112           53308\n",
       "1  U598644     N31055       448386           31370\n",
       "2  U532401     N31323       388825           31545"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uai_test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_train_path = dataset_path + f\"{dataset_size}_train.csv\"\n",
    "uai_train_df.to_csv(uai_train_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "uai_test_path = dataset_path + f\"{dataset_size}_test.csv\"\n",
    "uai_test_df.to_csv(uai_test_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actually **create the dok-matrix**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(575553, 76190)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users, num_articles = uai_train_df.user_id.nunique(), uai_train_df.article_id.nunique()\n",
    "num_users, num_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id,article_id,user_int_id,article_int_id\n",
      "\n",
      "U87243,N8668,564112,67478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_matrix = sp.dok_matrix((num_users, num_articles), dtype=np.float32)\n",
    "\n",
    "with open(uai_train_path, \"r\") as f:\n",
    "    header = f.readline()\n",
    "    line = f.readline()\n",
    "    print(header)\n",
    "    print(line)\n",
    "    while line != None and line != \"\":\n",
    "        line_list = line.split(\",\")\n",
    "        user, article = int(line_list[2]), int(line_list[3])\n",
    "        train_matrix[user, article] = 1.0\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pickle_matrix:\n",
    "    pickle.dump(train_matrix, open(dataset_path + \"train_matrix.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, when we want to evaluate our recommender systems, we need to **compare the ranking for the known -- but not learned -- interactions with known non-interactions**, so that we can tell how useful our recommendation is: the higher the ranking of the test interaction, the better our model! In order to do this, we want to **extract 99 non read articles for every user in our test set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_interactions = list(zip(uai_test_df.user_int_id, uai_test_df.article_int_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_negatives = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_interactions = []\n",
    "for u, i in test_interactions:\n",
    "    negatives = []\n",
    "    for t in range(num_negatives):\n",
    "        j = np.random.randint(num_articles)\n",
    "        while (u, j) in train_matrix.keys():\n",
    "            j = np.random.randint(num_articles)\n",
    "        negatives.append(j)\n",
    "    negative_interactions.append(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1936931, 1936931, 99)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_interactions), len(negative_interactions), len(negative_interactions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally**, we want to **write our one positive interaction along with the randomly generated non interactions into a tsv file** and we're done with the cleaning and preprocessing of the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = negative_interactions[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_interactions)):\n",
    "    output[i].insert(0, test_interactions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_negatives_path = dataset_path + f\"{dataset_size}_test_negatives.tsv\"\n",
    "with open(test_negatives_path, 'w') as f:\n",
    "    for line in output:\n",
    "        line_str = '\\t'.join(str(ele) for ele in line) + \"\\n\"\n",
    "        f.write(line_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
